{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modern Classification Workflow: Corporate Bankruptcy (2026 Best Practices)\n",
        "\n",
        "This notebook demonstrates **reproducibility**, **correct validation** (pipelines + stratified CV), **multiple metrics**, and **interpretability (SHAP)** on the Taiwan Corporate Bankruptcy dataset. It serves as the reference implementation for the portfolio—see [docs/BEST_PRACTICES.md](docs/BEST_PRACTICES.md) for detailed explanations of each practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Reproducibility: set the random seed\n",
        "\n",
        "**Why:** ML involves randomness (splits, bootstrap, stochastic algorithms). Fixing the seed ensures that anyone who runs this notebook gets the same results. Always call `set_seed` once at the top and pass `random_state` to every random component (splits, estimators, CV)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from portfolio_utils import set_seed\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load data and prepare target/features\n",
        "\n",
        "Data is loaded via the portfolio data loader (Kaggle API or local `data/`). We separate the target `Bankrupt?` from features and use only numeric columns. No preprocessing is fitted here—that happens inside the pipeline so we avoid leaking information from the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from portfolio_utils import load_bankruptcy\n",
        "\n",
        "df = load_bankruptcy()\n",
        "y = df[\"Bankrupt?\"]\n",
        "X = df.drop(columns=[\"Bankrupt?\"]).select_dtypes(include=[np.number])\n",
        "# Drop constant columns so SelectKBest (F-test) is well-defined\n",
        "constant_cols = [c for c in X.columns if X[c].nunique() <= 1]\n",
        "if constant_cols:\n",
        "    X = X.drop(columns=constant_cols)\n",
        "feature_names = list(X.columns)\n",
        "print(\"Shape:\", X.shape, \"Target balance:\");\n",
        "print(y.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Stratified train/test split\n",
        "\n",
        "**Why:** The dataset is imbalanced (few bankruptcies). Using `stratify=y` keeps the same class proportions in train and test so evaluation is fair. We hold out 20% for a final test set and never use it until the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Pipeline: preprocessing + model\n",
        "\n",
        "**Why:** A single pipeline ensures (1) the scaler and feature selector are fitted only on training data, (2) the same transformations are applied to test data, and (3) cross-validation fits preprocessing per fold—no leakage. We use StandardScaler, SelectKBest (top 30 features by F-statistic), and XGBClassifier. Tree-based models can work without scaling, but scaling is harmless and keeps the pattern reusable for non-tree models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba37ede5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import xgboost as xgb\n",
        "\n",
        "n_features = min(30, X_train.shape[1] - 1)\n",
        "pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"selector\", SelectKBest(f_classif, k=n_features)),\n",
        "    (\"estimator\", xgb.XGBClassifier(random_state=42)),\n",
        "])\n",
        "pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Cross-validation with multiple metrics\n",
        "\n",
        "**Why:** We evaluate the **entire pipeline** with `cross_validate` so that scaling and feature selection are refit on each fold's training portion. Reporting several metrics (accuracy, precision, recall, F1, ROC-AUC) gives a complete picture; for imbalanced data, F1 and ROC-AUC are more informative than accuracy alone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "scoring = [\"accuracy\", \"precision_weighted\", \"recall_weighted\", \"f1_weighted\", \"roc_auc_ovr\"]\n",
        "cv_results = cross_validate(pipeline, X_train, y_train, cv=5, scoring=scoring, n_jobs=-1)\n",
        "\n",
        "print(\"Cross-validation (5-fold stratified):\")\n",
        "for metric in scoring:\n",
        "    key = f\"test_{metric}\"\n",
        "    if key in cv_results:\n",
        "        mean_val = cv_results[key].mean()\n",
        "        std_val = cv_results[key].std()\n",
        "        print(f\"  {metric}: {mean_val:.4f} (+/- {std_val:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Fit on full training set and evaluate on holdout test set\n",
        "\n",
        "We fit the pipeline once on all training data, then predict on the held-out test set. We report classification report, confusion matrix, and ROC-AUC to show we care about more than accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Classification report (test set):\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"Not bankrupt\", \"Bankrupt\"]))\n",
        "print(\"Confusion matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"ROC-AUC (test):\", roc_auc_score(y_test, y_proba).round(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Interpretability: SHAP summary\n",
        "\n",
        "**Why:** SHAP (Shapley values) explains which features drove the model's predictions. For tree models we use `TreeExplainer` on the **final estimator** and the **transformed** training data (as the model sees it). We use a sample of 500 rows to keep runtime reasonable. The beeswarm plot shows feature importance and the direction of each feature's effect (red = higher value pushes prediction toward bankruptcy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "\n",
        "    # Pipeline: data flows through scaler -> selector -> estimator.\n",
        "    # SHAP needs the estimator and the data in the form the estimator sees.\n",
        "    estimator = pipeline.named_steps[\"estimator\"]\n",
        "    X_train_transformed = pipeline[\"selector\"].transform(pipeline[\"scaler\"].transform(X_train))\n",
        "    selected_names = np.array(feature_names)[pipeline[\"selector\"].get_support()].tolist()\n",
        "\n",
        "    sample_size = min(500, len(X_train_transformed))\n",
        "    X_sample = X_train_transformed[:sample_size]\n",
        "\n",
        "    explainer = shap.TreeExplainer(estimator, X_sample)\n",
        "    shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    shap.summary_plot(shap_values, X_sample, feature_names=selected_names, max_display=15, show=False)\n",
        "    plt.tight_layout()\n",
        "    plt.title(\"SHAP summary (class 1 = Bankrupt)\")\n",
        "    plt.show()\n",
        "except ImportError:\n",
        "    print(\"Install shap: pip install shap (or uv add shap)\")\n",
        "except Exception as e:\n",
        "    print(\"SHAP error:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "- **Reproducibility:** `set_seed(42)` and `random_state=42` everywhere.\n",
        "- **Validation:** One pipeline (scaler → selector → model), fitted only on train; `cross_validate` on the pipeline; stratified splits.\n",
        "- **Metrics:** Accuracy, precision, recall, F1, ROC-AUC and classification report on the holdout test set.\n",
        "- **Interpretability:** SHAP summary plot on the fitted model with transformed features.\n",
        "\n",
        "Apply these patterns across the portfolio—see [docs/BEST_PRACTICES.md](docs/BEST_PRACTICES.md) and [IMPROVEMENTS.md](IMPROVEMENTS.md)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
