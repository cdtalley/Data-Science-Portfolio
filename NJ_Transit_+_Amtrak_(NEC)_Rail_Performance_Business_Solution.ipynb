{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPvcKPTn8qpW"
      },
      "source": [
        "# NJ Transit + Amtrak (NEC) Rail Performance Business Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HSzSbQjSjti"
      },
      "source": [
        "The busiest passenger rail line in the United States, Amtrak also operates passenger rail service; together the NJ Transit and Amtrak operate nearly 750 trains across the NJ Transit rail network. Our data originates from here; https://www.kaggle.com/pranavbadami/nj-transit-amtrak-nec-performance. This data is organized into a monthly performance report of nearly every train trip on the rail network with the features; date, train ID, stop sequence, station train is traveling from, station ID of train origin, station train is traveling to, train destination ID, scheduled time, actual time, and delay in minutes. We will clean and analyze these features for further machine learning.\n",
        "\n",
        "The business solution proposed is to create a classification model that can predict when a train is going to be late or not based on the features given. Clustering can also be used to segment rail lines that are in need of increased infrastructure and attention to upgrades to better serve the rail lines and prevent congestion and commute time. The purpose would be to better inform riders in advance of potential delays and cancellation using data driven machine learning algorithms. This would decrease congestion at rail stations where delayed crowds can often stagnate and minimize customer complaints and refunds by giving timelier notice of potential issues. \n",
        "\n",
        "I will be using various models and learning methods such as supervised learning (logistic regression, gradient boosting, KNN classifier, decision tree, random forest, SelectKBest, PCA, GridSearchCV hyperparameter tuning), unsupervised learning (t-SNE, PCA, KMeans and DBSCAN clustering) and deep learning models to implement the business solution above.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkS7gSX3LwFT",
        "outputId": "e8d6ae74-925e-4465-a61c-58a60ab37769"
      },
      "source": [
        "from portfolio_utils import set_seed\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.over_sampling import SMOTE\n",
        "set_seed(42)\n",
        "\n",
        "# Load data if not already loaded (run this cell first)\n",
        "try:\n",
        "    df\n",
        "except NameError:\n",
        "    from portfolio_utils.data_loader import load_nj_transit\n",
        "    df = load_nj_transit()\n",
        "df.head(2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFgRuY4Yzp9x"
      },
      "source": [
        "# Exploratory Data Analysis and Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFVLZ9GCh5jJ",
        "outputId": "1c9f3556-940e-4d13-9045-5708bb45b9f1"
      },
      "source": [
        "# Examining the first five rows by calling pandas head function.\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>train_id</th>\n",
              "      <th>stop_sequence</th>\n",
              "      <th>from</th>\n",
              "      <th>from_id</th>\n",
              "      <th>to</th>\n",
              "      <th>to_id</th>\n",
              "      <th>scheduled_time</th>\n",
              "      <th>actual_time</th>\n",
              "      <th>delay_minutes</th>\n",
              "      <th>status</th>\n",
              "      <th>line</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-05-01</td>\n",
              "      <td>5543</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Newark Penn Station</td>\n",
              "      <td>107</td>\n",
              "      <td>Newark Penn Station</td>\n",
              "      <td>107</td>\n",
              "      <td>2020-05-01 23:38:00</td>\n",
              "      <td>2020-05-01 23:40:09</td>\n",
              "      <td>2.150000</td>\n",
              "      <td>departed</td>\n",
              "      <td>Raritan Valley</td>\n",
              "      <td>NJ Transit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-05-01</td>\n",
              "      <td>5543</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Newark Penn Station</td>\n",
              "      <td>107</td>\n",
              "      <td>Union</td>\n",
              "      <td>38105</td>\n",
              "      <td>2020-05-01 23:47:00</td>\n",
              "      <td>2020-05-01 23:47:01</td>\n",
              "      <td>0.016667</td>\n",
              "      <td>departed</td>\n",
              "      <td>Raritan Valley</td>\n",
              "      <td>NJ Transit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-05-01</td>\n",
              "      <td>5543</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Union</td>\n",
              "      <td>38105</td>\n",
              "      <td>Roselle Park</td>\n",
              "      <td>31</td>\n",
              "      <td>2020-05-01 23:50:00</td>\n",
              "      <td>2020-05-01 23:51:04</td>\n",
              "      <td>1.066667</td>\n",
              "      <td>departed</td>\n",
              "      <td>Raritan Valley</td>\n",
              "      <td>NJ Transit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-05-01</td>\n",
              "      <td>5543</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Roselle Park</td>\n",
              "      <td>31</td>\n",
              "      <td>Cranford</td>\n",
              "      <td>32</td>\n",
              "      <td>2020-05-01 23:55:00</td>\n",
              "      <td>2020-05-01 23:55:31</td>\n",
              "      <td>0.516667</td>\n",
              "      <td>departed</td>\n",
              "      <td>Raritan Valley</td>\n",
              "      <td>NJ Transit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-05-01</td>\n",
              "      <td>5543</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Cranford</td>\n",
              "      <td>32</td>\n",
              "      <td>Westfield</td>\n",
              "      <td>155</td>\n",
              "      <td>2020-05-01 23:59:00</td>\n",
              "      <td>2020-05-01 23:59:01</td>\n",
              "      <td>0.016667</td>\n",
              "      <td>departed</td>\n",
              "      <td>Raritan Valley</td>\n",
              "      <td>NJ Transit</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         date train_id  stop_sequence  ...    status            line        type\n",
              "0  2020-05-01     5543            1.0  ...  departed  Raritan Valley  NJ Transit\n",
              "1  2020-05-01     5543            2.0  ...  departed  Raritan Valley  NJ Transit\n",
              "2  2020-05-01     5543            3.0  ...  departed  Raritan Valley  NJ Transit\n",
              "3  2020-05-01     5543            4.0  ...  departed  Raritan Valley  NJ Transit\n",
              "4  2020-05-01     5543            5.0  ...  departed  Raritan Valley  NJ Transit\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InXou_QYqrUA"
      },
      "source": [
        "There are a few features of interest here, most notable the delay in minutes column which seems to be a derivative of the scheduled time and actual time features. To_id and from_id correlates to the station names in the to and from columns. The date column spans the month of May"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "futTKeOuxtp1",
        "outputId": "47ec0acf-a848-4dee-927d-56d127c3775d"
      },
      "source": [
        "# Examining DataFrame features data types.\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 98698 entries, 0 to 98697\n",
            "Data columns (total 13 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   date            98698 non-null  object \n",
            " 1   train_id        98698 non-null  object \n",
            " 2   stop_sequence   87172 non-null  float64\n",
            " 3   from            98698 non-null  object \n",
            " 4   from_id         98698 non-null  int64  \n",
            " 5   to              98698 non-null  object \n",
            " 6   to_id           98698 non-null  int64  \n",
            " 7   scheduled_time  87172 non-null  object \n",
            " 8   actual_time     98698 non-null  object \n",
            " 9   delay_minutes   87172 non-null  float64\n",
            " 10  status          98698 non-null  object \n",
            " 11  line            98698 non-null  object \n",
            " 12  type            98698 non-null  object \n",
            "dtypes: float64(2), int64(2), object(9)\n",
            "memory usage: 9.8+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJZv9bm0qz9x"
      },
      "source": [
        "Data types seem to be correct for all features, we will further clean our data by dropping unneccesary columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWmQT3EwyL49"
      },
      "source": [
        "# Dropping to_id and from_id columns from our DataFrame.\n",
        "df = df.drop(columns='to_id')\n",
        "df = df.drop(columns='from_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vsg4_KBOq9LE"
      },
      "source": [
        "Dropping redundant features such as to_id and from_id since our actual locations are listed in features to and from and can be used as dummy variables for easier interpretation. These will also not be useful for analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEW_CijWzcCk",
        "outputId": "30a6abbf-a981-441d-affc-0f5261231316"
      },
      "source": [
        "# Calling pandas .duplicated function to create boolean series of duplicate values and storing in a pandas DataFrame.\n",
        "duplicate_rows_df = df[df.duplicated()]\n",
        "print(duplicate_rows_df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x28Q9axprFQc"
      },
      "source": [
        "Checking DataFrame for rows containing duplicate data, we have only 11 duplicate rows out of 98,000 rows a negligible amount."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0LbI_oEzo56",
        "outputId": "499b3f2d-fb94-47d0-c635-77fcf5c1b36b"
      },
      "source": [
        "# Finding the percent of missing values by calling pandas .isna function which returns a mask of bool values for each element in \n",
        "# DataFrame that indicates whether an element is not an NA value, and rounding the mean.\n",
        "df.isna().mean().round(4) * 100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "date               0.00\n",
              "train_id           0.00\n",
              "stop_sequence     11.68\n",
              "from               0.00\n",
              "to                 0.00\n",
              "scheduled_time    11.68\n",
              "actual_time        0.00\n",
              "delay_minutes     11.68\n",
              "status             0.00\n",
              "line               0.00\n",
              "type               0.00\n",
              "dtype: float64"
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNVN7GB62kEX"
      },
      "source": [
        "Checking the percentage of missing data in each feature. Stop_sequence, scheduled_time and delay_minutes need to have their missing values dropped for further machine learning. \n",
        "These features are missing a significant amount of data, but for the most part our dataset is complete. We will drop the rows that are missing to save the intact information within them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suBc2wc6rZKD"
      },
      "source": [
        "# .dropna returns the DataFrame with missing values dropped.\n",
        "df = df.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqTyEyWs28GS",
        "outputId": "5b2eb282-6a4d-4375-efe5-9215149b8d26"
      },
      "source": [
        "# Confirming removal of missing values from DataFrame.\n",
        "df.isna().mean().round(4) * 100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "date              0.0\n",
              "train_id          0.0\n",
              "stop_sequence     0.0\n",
              "from              0.0\n",
              "to                0.0\n",
              "scheduled_time    0.0\n",
              "actual_time       0.0\n",
              "delay_minutes     0.0\n",
              "status            0.0\n",
              "line              0.0\n",
              "type              0.0\n",
              "dtype: float64"
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0KS1PvnreoM"
      },
      "source": [
        "Now that we have cleaned our data we can go ahead and visualize some of the features of interest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qazy04oINMrJ",
        "outputId": "f7a44e90-e62e-40ef-8b8e-e1f4c50af3f2"
      },
      "source": [
        "# Creating distribution plot with seaborn for 'delay_minutes' column.\n",
        "sns.histplot(df['delay_minutes'], kde=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKnngyN4rlto"
      },
      "source": [
        "This plot shows the distribution of our delay in minutes. Most delays were minor, with the highest density either having no delay or about a 5 minute delay. We will use this feature to create our target variable by creating a threshold that will alert to a delay greater than 15 minutes later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX-GMjGj49vH",
        "outputId": "af152901-1802-4970-fc71-1521e1d46e5a"
      },
      "source": [
        "# Countplot for 'status' with seaborn.\n",
        "sns.countplot(x ='status', data = df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTf6Sx77rtYE"
      },
      "source": [
        "This count plot shows train status distribution. A count plot is a histogram across a categorical, instead of quantitative, variable. A majority of trains departed in a timely manner, the trains listed estimated status means they have not explicity departed and may be running late.\n",
        "Canceled status indicates train has been canceled. We will one-hot encode these variables to run in our machine model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-39NxLK45AH1",
        "outputId": "b7a855cc-9842-4757-f1cd-058fc83a848a"
      },
      "source": [
        "# Countplot displaying train 'line' distribution with seaborn.\n",
        "plt.figure(figsize=(20,6))\n",
        "sns.countplot(x ='line', data = df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTMtbn2frzSu"
      },
      "source": [
        "The majority of train lines are coming from the North Jersey coast, the Morristown line, and the Northeastern Corridor train lines. We have the smallest amount of data for the Princeton Shuttle line. We will also one-hot encode these variables to feed into our machine model and determine any lines of interest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vuR9Sq1ovQs",
        "outputId": "aedd2b45-5ad1-4ee4-c6d5-f891f2f31311"
      },
      "source": [
        "# Defining figure size with pyplot.\n",
        "plt.figure(figsize=(12, 6))\n",
        "# Applying correlation scoring to dataframe.\n",
        "corr = df.apply(lambda x: pd.factorize(x)[0]).corr()\n",
        "# Creating heatmap with seaborn.\n",
        "ax = sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, \n",
        "                 linewidths=.2, cmap=\"YlGnBu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TOgOEDor7yF"
      },
      "source": [
        "The strongest correlation being displayed is the correlation between train line and delay in minutes, as well as the correlation between to and from destinations and delay in minutes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time Series Analysis\n",
        "\n",
        "Delay patterns are inherently temporal. We analyze **daily trend**, **day-of-week seasonality**, and **hour-of-day effects** to support scheduling, crew allocation, and proactive passenger alerts. All aggregations use the cleaned `df` (with non-null `delay_minutes` and `scheduled_time`) before dropping date/time columns for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Time series view: ensure datetime, then aggregate by date\n",
        "ts = df[[\"date\", \"scheduled_time\", \"delay_minutes\"]].copy()\n",
        "ts[\"date\"] = pd.to_datetime(ts[\"date\"], errors=\"coerce\")\n",
        "ts = ts.dropna(subset=[\"date\", \"delay_minutes\"])\n",
        "# Parse scheduled_time for hour (handle mixed formats)\n",
        "ts[\"scheduled_dt\"] = pd.to_datetime(ts[\"scheduled_time\"], errors=\"coerce\")\n",
        "ts[\"hour\"] = ts[\"scheduled_dt\"].dt.hour\n",
        "ts[\"day_of_week\"] = ts[\"date\"].dt.dayofweek  # 0=Monday\n",
        "\n",
        "daily = ts.groupby(\"date\")[\"delay_minutes\"].agg([\"mean\", \"median\", \"count\"]).reset_index()\n",
        "daily[\"rolling_mean_7\"] = daily[\"mean\"].rolling(7, min_periods=1).mean()\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
        "axes[0].plot(daily[\"date\"], daily[\"mean\"], alpha=0.6, label=\"Daily mean delay (min)\")\n",
        "axes[0].plot(daily[\"date\"], daily[\"rolling_mean_7\"], color=\"C1\", lw=2, label=\"7-day rolling mean\")\n",
        "axes[0].set_ylabel(\"Delay (minutes)\")\n",
        "axes[0].set_title(\"Daily Average Delay and 7-Day Rolling Mean\")\n",
        "axes[0].legend(loc=\"upper right\")\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].fill_between(daily[\"date\"], daily[\"count\"], alpha=0.5, color=\"steelblue\")\n",
        "axes[1].set_ylabel(\"Trip count\")\n",
        "axes[1].set_xlabel(\"Date\")\n",
        "axes[1].set_title(\"Daily Trip Volume\")\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Day-of-week and hour-of-day seasonality\n",
        "dow = ts.groupby(\"day_of_week\")[\"delay_minutes\"].mean().reset_index()\n",
        "dow[\"day_name\"] = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
        "hourly = ts.dropna(subset=[\"hour\"]).groupby(\"hour\")[\"delay_minutes\"].mean().reset_index()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "sns.barplot(data=dow, x=\"day_name\", y=\"delay_minutes\", color=\"steelblue\", ax=axes[0])\n",
        "axes[0].set_xlabel(\"Day of week\")\n",
        "axes[0].set_ylabel(\"Mean delay (minutes)\")\n",
        "axes[0].set_title(\"Mean Delay by Day of Week\")\n",
        "axes[0].tick_params(axis=\"x\", rotation=45)\n",
        "\n",
        "sns.lineplot(data=hourly, x=\"hour\", y=\"delay_minutes\", marker=\"o\", ax=axes[1])\n",
        "axes[1].set_xlabel(\"Hour of day (scheduled)\")\n",
        "axes[1].set_ylabel(\"Mean delay (minutes)\")\n",
        "axes[1].set_title(\"Mean Delay by Hour of Day\")\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Heatmap: day of week x hour of day (mean delay in minutes)\n",
        "pivot = ts.dropna(subset=[\"hour\"]).pivot_table(\n",
        "    values=\"delay_minutes\", index=\"day_of_week\", columns=\"hour\", aggfunc=\"mean\"\n",
        ")\n",
        "day_labels = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
        "pivot.index = [day_labels[i] for i in pivot.index]\n",
        "plt.figure(figsize=(14, 5))\n",
        "sns.heatmap(pivot, cmap=\"YlOrRd\", annot=False, fmt=\".1f\", cbar_kws={\"label\": \"Mean delay (min)\"})\n",
        "plt.xlabel(\"Hour of day (scheduled)\")\n",
        "plt.ylabel(\"Day of week\")\n",
        "plt.title(\"Mean Delay by Day of Week and Hour of Day (Time Series View)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Time series takeaways:** Daily rolling mean smooths noise and can inform schedule padding. Day-of-week and hour-of-day patterns support crew and resource planning; the heatmap highlights peak-delay windows for targeted interventions. For production forecasting, consider `TimeSeriesSplit` for validation and autoregressive or lag-based features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXgjTorTsDLi"
      },
      "source": [
        "# Dropping the scheduled time and actual time columns since we are interested in the amount of delay which is measured in the delay feature having been already extracted for us.\n",
        "df = df.drop(columns='scheduled_time')\n",
        "df = df.drop(columns='actual_time')\n",
        "df = df.drop(columns='date')\n",
        "\n",
        "# We can drop our type feature as after dropping missing values it only contains one unique value and won't contribute to our DataFrame.\n",
        "df = df.drop(columns='type')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNciJbGhsbYr"
      },
      "source": [
        "# Creating new column for our target variable that consists of booleans for 'delay_minutes' values greater than 15.\n",
        "df['late'] = df['delay_minutes']>15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu7_Gyf9gqz5"
      },
      "source": [
        "# Dropping 'delay_minutes' column from DataFrame.\n",
        "df = df.drop(columns='delay_minutes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ljr6DkpskcV"
      },
      "source": [
        "We can now drop our delay column as we have extracted the necessary info from it by creating a target variable derived from it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMxaEe5283F_"
      },
      "source": [
        "# Supervised Learning with SelectKBest Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v10A1U7U1Ht"
      },
      "source": [
        "# Create our target variable y.\n",
        "y = df['late']\n",
        "\n",
        "# Create X values by dropping our target.\n",
        "X = df.drop(columns=['late'])\n",
        "\n",
        "# Use pandas get_dummies function to create dummy variables necessary for statistical interpretation (One Hot Encoding).\n",
        "X = pd.get_dummies(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "S7rgzUhSSUAh",
        "outputId": "17945c46-d909-4690-a4a2-4e03df80b897"
      },
      "source": [
        "# Creating a countplot for our target variable y with seaborn.\n",
        "sns.countplot(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cqx_WAJswhCd"
      },
      "source": [
        "Examining the distribution of our target variable to see if our classes are balanced for best performance in supervised machine learning models.\n",
        "The countplot shows that the target class is imbalanced, with our target value of delay greater than 15 minutes having as expected less True values. We must fix this with SMOTE (Synthetic Minority Oversampling Technique). SMOTE synthetically oversamples the minority target class to match when sampling strategy is set to minority."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8MGbk_RDMx9"
      },
      "source": [
        "# Creating oversampler with minority sampling strategy.\n",
        "sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
        "\n",
        "# Fitting SMOTE to X and y variables.\n",
        "X, y = sm.fit_resample(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "7XG-pEdXSvMP",
        "outputId": "f9a4b2ea-569d-460b-fa30-af160af9fab0"
      },
      "source": [
        "# Creating another countplot for our target variable y with seaborn.\n",
        "sns.countplot(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k0BPAc8O_K5"
      },
      "source": [
        "Re-examining y distribution to confirm classes have been balanced by SMOTE. Our distribution of True values now matches that of False values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nfs4Z4HTDH7G"
      },
      "source": [
        "# Putting X variable back into DataFrame after resampling for feature selection.\n",
        "X = pd.DataFrame(X)\n",
        "\n",
        "# Fitting SelectKBest feature selector to our variables using f_classif for our classification task with a k-value of 200.\n",
        "selector = SelectKBest(f_classif, k=200).fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMugHAN6yp3v"
      },
      "source": [
        "We will select the 200 best features based on highest k scores to reduce runtime and retain information. Reducing k-value to a number that is too low will cause overfitting due to loss of too much information. If we set a k value that is too high it will not reduce dimensionality therefore no impact on performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59NSQ4xLy5yv"
      },
      "source": [
        "# Creating boolean values by calling get_support on selector.\n",
        "boolean = selector.get_support()\n",
        "\n",
        "# Pulling newly selected columns into a new X variable with booleans.\n",
        "X_new = X[X.columns[boolean]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cLfq27Ey6U1"
      },
      "source": [
        "Our new X variable now consists of our 200 selected best features based on K values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbWuErbkNo7h",
        "outputId": "92ac32ce-97bb-42c8-eb21-a53b15221677"
      },
      "source": [
        "# Checking shape property of original X variable.\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz4HSWKWNtO3",
        "outputId": "463b70d2-788f-47de-fb55-f1130a608846"
      },
      "source": [
        "# Checking shape property of new X variable.\n",
        "X_new.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQuHx4sYnyCZ"
      },
      "source": [
        "Examining newly selected feature set and confirming dimensionality reduction. We've successfully reduced our DataFrame by 590 columns, this will dramatically reduce run time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA4JxxnGzV2v"
      },
      "source": [
        "# Standardize features by removing the mean and scaling to unit variance with sklearns StandardScaler.\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit scaler to data by calling fit_transform on our new data.\n",
        "scaled_data = scaler.fit_transform(X_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePTW82N9zW6M"
      },
      "source": [
        "Scaling newly transformed features for machine learning model processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa7gUYlckIZE"
      },
      "source": [
        "# Using sklearns train_test_split to split data into random train and test subsets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.20, stratify=y, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cmzDg2szepa"
      },
      "source": [
        "Now that we have finished processing, cleaning, and selecting our data features we are able to feed them into our supervised learning machine models and measure their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUWGoo13gHYe"
      },
      "source": [
        "## Logistic Regression Classifier with SelectKBest Selected Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1ahY5CzcXU-r",
        "outputId": "c29845e8-5d24-41a4-8335-4a7681f41fb6"
      },
      "source": [
        "# Timing our model.\n",
        "start_time = time.time()\n",
        "\n",
        "# Adjust our max iteration value for proper convergence.\n",
        "clf = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
        "\n",
        "# Print model time. \n",
        "print(\"--- %s seconds ---\" % round(time.time() - start_time, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqoZCPJJzxlt"
      },
      "source": [
        "Setting model run timer to print time upon completion, while adjusting the max iteration value for proper convergence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BvxmZmhvYli4",
        "outputId": "aa565c79-b526-4fe5-8e2a-303be69aa9b3"
      },
      "source": [
        "# Print cross validation score.\n",
        "cross_val_score(clf, X_train, y_train, cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSJQImKS0Ln2"
      },
      "source": [
        "The cross validation score for the Logistic Regression Classifier is consistent and doesn't show any signs of over or underfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YTGyf5NlY8Ds",
        "outputId": "fda0db1d-9b7e-4d13-e225-7303eba27069"
      },
      "source": [
        "# Print classification report for our Logistic Regression classifier.\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test,y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Classification report:\")\n",
        "print(report)\n",
        "print(\"Confusion matrix:\")\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IY0oNNK0d3R"
      },
      "source": [
        "The classification report for our Logistic Regression classifier is showing consistent values and does not show signs of fitment issues. Logistic Regression classification performs best on binary outputs so it is not surprising this model works okay on our data. Performance has much room for improvement however."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFNiCMtIXUqd"
      },
      "source": [
        "## Gradient Boosting Classifier with SelectKbest Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4TNcrJy-fxM0",
        "outputId": "121f41ef-4920-4e2f-d099-285aa8ff00e5"
      },
      "source": [
        "# Timing our model.\n",
        "start_time = time.time()\n",
        "\n",
        "# Fitting gradient boosting classifier to training data. \n",
        "clf = GradientBoostingClassifier().fit(X_train, y_train)\n",
        "\n",
        "# Print model run time.\n",
        "print(\"--- %s seconds ---\" % round(time.time() - start_time, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_1dPI2zMf4Ag",
        "outputId": "362389ee-de7c-46bd-cd94-c0df18613e99"
      },
      "source": [
        "# Print cross validation score.\n",
        "cross_val_score(clf, X_train, y_train, cv=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtPOpQbj1q9T"
      },
      "source": [
        "The cross validation score for the Gradient Boosting Classifier is consistent in the .88 range and doesn't show any signs of over or underfitting. This means our model should be expected to handle new data, since it is not overtraining on the training data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oOldJ2k9f6TH",
        "outputId": "9ce7523c-20e5-4c6f-fe6c-8b3a0c75bbb9"
      },
      "source": [
        "# Print classification report for our Gradient Boosting classifier.\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test,y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Classification report:\")\n",
        "print(report)\n",
        "print(\"Confusion matrix:\")\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnJifFkM10in"
      },
      "source": [
        "The classification report for our Gradient Boosting Classifier is also showing consistent values and does not show evidence of fitment issues. We are seeing slightly higher precision here than for our previous logistic regression model although at double the run time. Additional research could include hyperparameter tuning the learning rate and number of estimators to improve accuracy or run time since we did not include any hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p-fHgqGsodi"
      },
      "source": [
        "## KNN Classifier with SelectKBest Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGZpM0N1soDi",
        "outputId": "52986091-3889-4eee-f2cf-93064fc8155b"
      },
      "source": [
        "# Timing our model.\n",
        "start_time = time.time()\n",
        "\n",
        "# Creating KNN classifier with n_neighbors set to 25 to establish performance. \n",
        "knn = KNeighborsClassifier(n_neighbors=25)\n",
        "\n",
        "# Fitting KNN classifier to training data. \n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "print(\"--- %s seconds ---\" % round(time.time() - start_time, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhZcVypy2M4a"
      },
      "source": [
        "Model run time is much better than all previous models. n_neighbors hyperparameter was set to 25 as a base parameter, this may be tuned in further research to optimize KNN classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Nm5UwWdtasc",
        "outputId": "538ba304-4b39-44d8-dc35-aedd988fe396"
      },
      "source": [
        "# Print cross validation score for training variables.\n",
        "cross_val_score(knn, X_train, y_train, cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooC0v8H72Y6V"
      },
      "source": [
        "The cross validation score for the KNN Classifier is consistent and doesn't show any signs of over or underfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBH8CH4RtsVr",
        "outputId": "c4b989a3-b537-4ea1-c9ea-9bf165f52b0c"
      },
      "source": [
        "# Print classification report.\n",
        "y_pred = knn.predict(X_test)\n",
        "accuracy = accuracy_score(y_test,y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Classification report:\")\n",
        "print(report)\n",
        "print(\"Confusion matrix:\")\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU9inGJD2nPl"
      },
      "source": [
        "The classification report for our KNN Classifier is also showing consistent values and does not show evidence of fitment issues. Weighted averages are also higher as well as F1 score. We are seeing much better model performance here in all aspects than the previous gradient boosting and logistic regression models even without any hyperparameter tuning. KNN classification works well on dimensionally reduced data and runs fast as it does not require any prior training also known as lazy learning. Lazy learning is a machine learning method in which generalization of the training data is delayed until a query is made to the system, as opposed to eager learning, where the system tries to generalize the training data before receiving queries. This reduces the computational complexity needed to run the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj_DqjjQvOB_"
      },
      "source": [
        "## Decision Tree Classifier with SelectKBest Selected Features and Optimized Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPqlRWGG-DsZ",
        "outputId": "1acac851-fe0d-4dbc-b7fd-67dd007d2c96"
      },
      "source": [
        "# Timing our model.\n",
        "start_time = time.time()\n",
        "\n",
        "# Building tree.\n",
        "decision_tree = tree.DecisionTreeClassifier()\n",
        "\n",
        "# Fit tree to training data.\n",
        "decision_tree.fit(X_train, y_train)\n",
        "\n",
        "# Print model run time.\n",
        "print(\"--- %s seconds ---\" % round(time.time() - start_time, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBtwQkeB25l1"
      },
      "source": [
        "We immediately see that decision tree model run time is much better than all previous models and half the time of our KNN classification model even without any hyperparameter tuning which we will perform in the followin cells. Binary decision trees are made through a process known as binary recursive partitioning. This is an iterative process of splitting the data into partitions, and then splitting it up further on each of the branches. This process also helps clean up unnecessary features and results in a fast and inexpensive model to run with performance equal to other classification techniques that require more compuational intensity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QHgeba0DvUeW",
        "outputId": "927d5494-5109-4b31-d3b9-4c185c1f5fdb"
      },
      "source": [
        "# Create tree parameter grid adjusting combination of criterion, max_depth, and max_features for best model performance.\n",
        "tree_para = {'criterion':['gini','entropy'],\n",
        "             'max_depth':[5,10,50,100,500],\n",
        "             'max_features':[5,10,50,100,200]}\n",
        "          \n",
        "# Exhaustive search over specified parameter values using GridSearchCV. GridSearchCV will score all possible combinations of hyperparameters.\n",
        "clf = GridSearchCV(decision_tree, tree_para, cv=5)\n",
        "\n",
        "# Fit tree to training data using GridSearchCV.\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Append GridSearchCV results to new DataFrame for easier interpretation.\n",
        "gscv_df = pd.DataFrame(clf.cv_results_)\n",
        "\n",
        "# Sort values by test score rank to find hyperparameters with the highest score.\n",
        "gscv_df.sort_values('rank_test_score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfGiYiYl4Isn"
      },
      "source": [
        "Applying GridSearchCV to find optimal hyperparameters to improve our classification report. We are looking at a few parameters to see what works best with our model by implementing them into a parameter grid. More complex parameter grids require increased computation time. Finally we will be putting our results into a Pandas DataFrame for easier interpretation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaK0gm67v1DX",
        "outputId": "47c2aad7-c91c-44a5-8cc3-d0798cdf3d39"
      },
      "source": [
        "# Start timing model.\n",
        "start_time = time.time()\n",
        "\n",
        "# Build decision tree using hyperparameters with the highest ranked test score.\n",
        "decision_tree = tree.DecisionTreeClassifier(\n",
        "    criterion='entropy',\n",
        "    max_depth=500,\n",
        "    max_features=200,\n",
        ")\n",
        "\n",
        "# Fit tree to training data.\n",
        "decision_tree.fit(X_train, y_train)\n",
        "\n",
        "print(\"--- %s seconds ---\" % round(time.time() - start_time, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pfc3dJ_3KjS"
      },
      "source": [
        "We see a slight improvement in run time with optimized hyperparameters reducing run time by half a second."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XulDeYTdv6OX",
        "outputId": "6eff471a-bddc-4ff0-dbf4-0ce634636cfb"
      },
      "source": [
        "# Print cross validation score.\n",
        "cross_val_score(decision_tree, X_train, y_train, cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ0noLGN3ZQG"
      },
      "source": [
        "The cross validation score for the Decision Tree Classifier is consistent and doesn't show any signs of over or underfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4hwQQ3sv6pk",
        "outputId": "b5e57cab-0769-430a-d6b0-456c60d1058d"
      },
      "source": [
        "# Print classificiation report. \n",
        "y_pred = decision_tree.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test,y_pred)\n",
        "decision_tree_report = classification_report(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy: \", dt_accuracy)\n",
        "print(\"Classification report:\")\n",
        "print(decision_tree_report)\n",
        "print(\"Confusion matrix:\")\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBCmY_YD3hj0"
      },
      "source": [
        "Model performance is much better than our previous models in all aspects with no signs of overfitting. We have reduced run time to 4.3 seconds and increased the accuracy score. Decision tree models are very fast and usually not computationally intensive as they do not compute all possibilities for a tree, and optimize which data to split in the given nodes. As the decision tree splits data further its speed increases also as it iterates over a smaller subset of data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGjqB8V2v8wD"
      },
      "source": [
        "## Random Forest Classifier with SelectKBest Selected Features and Optimized Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x5MhVsdPYWf",
        "outputId": "3ea2d68b-5f9f-43bc-efe7-525a66af0c7d"
      },
      "source": [
        "# Timing our model.\n",
        "start_time = time.time()\n",
        "\n",
        "# Building random forest classifier.\n",
        "rfc = ensemble.RandomForestClassifier()\n",
        "\n",
        "# Fitting classifier to training data. \n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "# Print model runtime.\n",
        "print(\"--- %s seconds ---\" % round(time.time() - start_time, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH9zZEDB30XD"
      },
      "source": [
        "Setting up Random Forest Classifier by fitting to data and setting model run timer to print time upon completion. Model run time is much longer than the previous models. This is because random forest builds multiple decision trees and merges them together requiring more computational intensity than a single tree. We will try to reduce the run time with hyperparameter tuning below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0Ons7OvgwP5j",
        "outputId": "8b427634-28df-4e54-cdc3-ca152fead61d"
      },
      "source": [
        "# Creating hyperparameter grid.\n",
        "param_grid = { \n",
        "    'n_estimators': [5,50,100,500,1000],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'max_depth' : [4,6,8,10],\n",
        "    'criterion' :['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Setting up GridSearchCV to run on our random forest classifier.\n",
        "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5)\n",
        "\n",
        "# Fitting GridSearchCV to training data. \n",
        "CV_rfc.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Etg0RlKu4CRD"
      },
      "source": [
        "We are looking at a few parameter combinations to see what works best with our model by implementing them into a parameter grid once again. More complex parameter grids require increased computation time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPeay9TOwW9T",
        "outputId": "539c394f-b2aa-4464-a98c-a78ada0538be"
      },
      "source": [
        "# Calling GridSearchCV .best_params_ to pull best parameters.\n",
        "CV_rfc.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcAgrsC84ye_"
      },
      "source": [
        "This time we will find the best parameters by calling '.best_params_' instead of sorting by test rank score in a DataFrame. This will pull the best parameters from GridSearchCV also."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZxbfeWDwXa1",
        "outputId": "7ce96d6f-7284-481e-e22e-4de7cd0aedda"
      },
      "source": [
        "# Timing our model.\n",
        "start_time = time.time()\n",
        "\n",
        "# Building random forest classifier with best parameters from GridSearchCV.\n",
        "rfc = ensemble.RandomForestClassifier(criterion='entropy', \n",
        "                                      max_depth=10,\n",
        "                                      max_features='sqrt',\n",
        "                                      n_estimators=5, random_state=42)\n",
        "# Fitting RFC with new hyperparameters.\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "# Print model run time.\n",
        "print(\"--- %s seconds ---\" % round(time.time() - start_time, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bKn8se05IzL"
      },
      "source": [
        "Creating our model using optimized hyperparameters from GridSearchCV. We see a dramatic decrease in model run time from 50 seconds to just over 1 second."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU-lvDtswavh",
        "outputId": "d09d021f-d7b2-41c9-b48c-f03e6afab825"
      },
      "source": [
        "# Print cross-validation score.\n",
        "cross_val_score(rfc, X_train, y_train, cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6soFUZ605iJI"
      },
      "source": [
        "Cross-validation score shows consistent scoring and no signs of overfitting or underfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM0GMJ5VwchR",
        "outputId": "7616e97e-ac39-4e40-d213-e8ab5c4d5d40"
      },
      "source": [
        "# Print classification report.\n",
        "y_pred = rfc.predict(X_test) \n",
        "rfc_accuracy = accuracy_score(y_test,y_pred)\n",
        "rfc_report = classification_report(y_test, y_pred)\n",
        "rfc_cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Classification report:\")\n",
        "print(\"Accuracy: \", rfc_accuracy)\n",
        "print(rfc_report)\n",
        "print(\"Confusion matrix:\")\n",
        "print(rfc_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXk_AQvy5its"
      },
      "source": [
        "Model performance is not as good as the previous decision tree model in all aspects; precision, recall, f1-score, accuracy, and their respective averages. Once again no signs of overfitting or underfitting. We significantly decreased our run time with hyperparameter tuning using GridSearchCV to perform better than our Decision Tree Classifier however with a lower scoring.\n",
        "\n",
        "As shown here ensemble models are not always better performers than their individual counterparts, they are better if the single decision tree models suffer from instability most usually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjfoys3UOQnq"
      },
      "source": [
        "### Supervised Learning with SelectKBest: Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAS4pmmpeRSk"
      },
      "source": [
        "In conclusion, decision tree model performance was best out of all other models (logistic regression, gradient boosting, KNN, random forest) using SelectKBest selected features with the best model run time and performance metrics. I hypothesize that this is due to our dimensionality reduction technique SelectKBest that allowed our decision tree to perform better than our random forest model. Without feature selection we most likely would have seen the opposite effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASx5PcHrkb-P"
      },
      "source": [
        "# Supervised Learning with PCA Feature Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bexMnnXkkzB"
      },
      "source": [
        "# Create our target variable y.\n",
        "y = df['late']\n",
        "\n",
        "# Create X values by dropping our target.\n",
        "X = df.drop(columns=['late'])\n",
        "\n",
        "# Use pandas get_dummies function to create dummy variables necessary for statistical interpretation (One Hot Encoding).\n",
        "X = pd.get_dummies(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "ZHL5mYxwBfXm",
        "outputId": "f1cc503d-98d7-4cb9-c8be-834b40ce0074"
      },
      "source": [
        "# Creating a countplot for our target variable y with seaborn.\n",
        "sns.countplot(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M0QvHKh60uY"
      },
      "source": [
        "Examining initial target class to check balancing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlxHTmEsBc_u"
      },
      "source": [
        "# Creating oversampler with minority sampling strategy.\n",
        "sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
        "\n",
        "# Fitting SMOTE to X and y variables.\n",
        "X, y = sm.fit_resample(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "bp0ryvtPBf8s",
        "outputId": "fd087867-6df6-42ce-d95d-11a4322ee559"
      },
      "source": [
        "# Creating another countplot for our target variable y with seaborn.\n",
        "sns.countplot(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeyEvVTf7MYc"
      },
      "source": [
        "Class balancing using SMOTE and confirmation of class balance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOLYfG_i7PSW"
      },
      "source": [
        "# Standardize features by removing the mean and scaling to unit variance with sklearns StandardScaler.\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit scaler to data by calling fit_transform on our data.\n",
        "scaled_data = scaler.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9stqrdVu7PoP"
      },
      "source": [
        "First we must scale our data so that it can be fit into our PCA model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hcxJcXt7V1s"
      },
      "source": [
        "# PCA feature reduction with the n_components set to represent 50% of the variance in our dataset.\n",
        "sklearn_pca = PCA(n_components = 0.50)\n",
        "\n",
        "# Apply PCA to our scaled data.\n",
        "X_pca = sklearn_pca.fit_transform(scaled_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODu1VS9qlC59",
        "outputId": "b1ac9a43-2fd7-473a-a911-a7011c8d95e2"
      },
      "source": [
        "# Checking shape property of original X variable.\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adX7vlIQlDYA",
        "outputId": "10c96d27-4265-4171-9e1a-37e4d2dff6f8"
      },
      "source": [
        "# Checking shape property of new X variable.\n",
        "X_pca.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YORivXee7n3s"
      },
      "source": [
        "Examing newly created PCA decomposed features. We successfully reduced DataFrame by 528 features to account for 50% of the variance in our original dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GdOMipjBR2E"
      },
      "source": [
        "# Using sklearns train_test_split to split data into random train and test subsets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, stratify=y, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbQQqRz_62gC"
      },
      "source": [
        "## Logistic Regression Classifier with PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "av5AsdrK62gC",
        "outputId": "00521ad8-d0b6-4392-8514-8ee8d9b9d6f9"
      },
      "source": [
        "# Timing our logistic regression classifier model.\n",
        "start_time = time.time()\n",
        "\n",
        "# Adjusting max iteration value for proper convergence.\n",
        "clf = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
        "\n",
        "# Print model runtime.\n",
        "print(\"--- %s seconds ---\" % round(time.time() - start_time, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy-94rjg62gE",
        "outputId": "ff28a176-ffbb-4813-c96e-f4c9d74e29ff"
      },
      "source": [
        "# Print the cross validation score.\n",
        "cross_val_score(clf, X_train, y_train, cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u03p2ahN7rlT"
      },
      "source": [
        "The cross validation score for the logistic regression classifier is showing consistent scoring and no signs of overfitting or underfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1di2nhz62gG",
        "outputId": "c4e73c23-ca89-454b-ae43-707dae2ec695"
      },
      "source": [
        "# Print classification report.\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test,y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Classification report:\")\n",
        "print(report)\n",
        "print(\"Confusion matrix:\")\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmFZqNjp8HT6"
      },
      "source": [
        "The classification report for our logistic regression classifier shows no signs of overfitting. Recall for false values and precision for true values decreased performance compared to SelectKBest, but we are seeing performance improvement in the weighted averages of precision, recall, and f-1 score as well as a 1% improvement in accuracy score. Most remarkable is the runtime reduction from aroudn 50 seconds to around 10 seconds even though the number of dimensions actually increased with PCA feature decomposition (262 features for PCA versus 200 for SelectKBest)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWKf68opBk1o"
      },
      "source": [
        "## Gradient Boosting Classifier with PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k49sP8s1Bk1p",
        "outputId": "5cce6258-e97f-4db7-890a-f41cf6693fcd"
      },
      "source": [
        "# Timing our model.\n",
        "start_time = time.time()\n",
        "\n",
        "# Fitting gradient boosting classifier to training data. \n",
        "clf = GradientBoostingClassifier().fit(X_train, y_train)\n",
        "\n",
        "# Print model run time.\n",
        "print(\"--- %s seconds ---\" % round(time.time() - start_time, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udorVMJW8wi4"
      },
      "source": [
        "Timing our gradient boosting classifier, we have a significantly longer run time than the logistic regression classifier as well as our gradient boosting classifier that we used with SelectKBest. This is due to the higher dimensionality of the PCA data so the gradient boosting classifier must form more trees during modeling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNhHm_AQBk1s",
        "outputId": "61e84818-28a4-4610-af56-9f9b03528d95"
      },
      "source": [
        "# Print cross validation score.\n",
        "cross_val_score(clf, X_train, y_train, cv=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpXhLHVL-PI7"
      },
      "source": [
        "Cross validation score for Gradient Boosting Classifier is consistent and shows no signs of overfitting or underfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsglD-vIBk1v",
        "outputId": "19816fc9-f4f1-4de9-cbd0-b7e734c106f3"
      },
      "source": [
        "# Print classification report.\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test,y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Classification report:\")\n",
        "print(report)\n",
        "print(\"Confusion matrix:\")\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZOG2Sq2-Pxg"
      },
      "source": [
        "We have slight performance increase of about .02 percent in weighted averages in all areas compared to logistic regression but at the cost of a much longer run time. The increased dimensionality of our PCA data also improved scoring in comparison to SelectKBest data but also increasing run time by almost 10x. The trade off in run time for a small performance increase seems to make this model an unlikely choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n1Bfb90Btxz"
      },
      "source": [
        "## KNN Classifier with PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg3cQgnLBtx0",
        "outputId": "994669dd-1e8b-4617-925d-d75be32fddf3"
      },
      "source": [
        "# Timing our model.\n",
        "start_time = time.time()\n",
        "\n",
        "# Creating KNN classifier with n_neighbors set to 25 to establish performance. \n",
        "knn = KNeighborsClassifier(n_neighbors=25)\n",
        "\n",
        "# Fitting KNN classifier to training data. \n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Print model runtime.\n",
        "print(\"--- %s seconds ---\" % round(time.time() - start_time, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9okIlknBtx2",
        "outputId": "0ad79497-96c9-43fa-81f2-3b3c951573c3"
      },
      "source": [
        "# Print cross-validation score.\n",
        "cross_val_score(knn, X_train, y_train, cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58plbpSWNvln"
      },
      "source": [
        "Cross validation score is consistent and shows no signs of fitment issues."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXOe73RoBtx4",
        "outputId": "fad8feb6-f1c8-4bfa-a77e-3ac1a7e8b920"
      },
      "source": [
        "# Printing classification report with test variables and prediction.\n",
        "y_pred = knn.predict(X_test)\n",
        "accuracy = accuracy_score(y_test,y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Classification report:\")\n",
        "print(report)\n",
        "print(\"Confusion matrix:\")\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QRoAj6FRCEC"
      },
      "source": [
        "Classification report for KNN Classifier is much better than our previous models with a higher weighted average in all categories. KNN was able to process PCA reduced data in a faster time and with better scoring than SelectKBest selected features despite having a higher dimensionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKLUSmHGB3su"
      },
      "source": [
        "## Decision Tree Classifier with PCA Decomposed Features and Optimized Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2SdHHbFB3sv",
        "outputId": "0a8627d1-e1c6-480e-c5bc-a6d5076564a5"
      },
      "source": [
        "# Timing our model.\n",
        "start_time = time.time()\n",
        "\n",
        "# Building tree.\n",
        "decision_tree = tree.DecisionTreeClassifier()\n",
        "\n",
        "# Fit tree to training data.\n",
        "decision_tree.fit(X_train, y_train)\n",
        "\n",
        "# Print model run time.\n",
        "print(\"--- %s seconds ---\" % round(time.time() - start_time, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rakUY6yiRESY"
      },
      "source": [
        "Timing and training our initial tree before running GridSearchCV. We have a signficant increase in run time compared to our decision tree classifier with SelectKBest selected features due to increased dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZgrH603KB3sx",
        "outputId": "aadb51d5-742a-412b-a3dd-d962d5487cab"
      },
      "source": [
        "# Create parameter grid for GridSearchCV.\n",
        "tree_para = {'criterion':['gini','entropy'],\n",
        "             'max_depth':[10,100,500,1000],\n",
        "             'max_features':[10,50,100,200]}\n",
        "\n",
        "# Create decision tree classifier with GridSearchCV.\n",
        "clf = GridSearchCV(decision_tree, tree_para, cv=5)\n",
        "\n",
        "# Fitting decision tree classifier to training data.\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Putting GridSearchCV results into Pandas DataFrame.\n",
        "gscv_df = pd.DataFrame(clf.cv_results_)\n",
        "\n",
        "# Sorting DataFrame by highest ranked test score.\n",
        "gscv_df.sort_values('rank_test_score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ffc8p9-RPou"
      },
      "source": [
        "The best performance was seen using the hyperparameters criterion: gini, max_depth: 500, and max_features: 200. We will use these hyperparameters to train our next decision tree model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8GmPCKDB3sz",
        "outputId": "9668f81e-848c-408c-9f26-22e0818ba6fe"
      },
      "source": [
        "# Timing our model.\n",
        "start_time = time.time()\n",
        "\n",
        "# Creating decision tree using new hyperparameters.\n",
        "decision_tree = tree.DecisionTreeClassifier(\n",
        "    criterion='gini',\n",
        "    max_depth=500,\n",
        "    max_features=200,\n",
        ")\n",
        "\n",
        "# Fitting tree to training data.\n",
        "decision_tree.fit(X_train, y_train)\n",
        "\n",
        "# Print model runtime.\n",
        "print(\"--- %s seconds ---\" % round(time.time() - start_time, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SleiweU_SFj3"
      },
      "source": [
        "Timing and training our tree using best parameters from GridSearchCV, we were able to decrease the model run time by over 10 seconds. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rVbzqj2eB3s0",
        "outputId": "101fefce-4835-4283-9f21-7c9802f3b350"
      },
      "source": [
        "# Print cross validation score.\n",
        "cross_val_score(decision_tree, X_train, y_train, cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC9aLjR_SNuq"
      },
      "source": [
        "Our decision tree is showing consistent scoring with no signs of underfitting or overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DlhCLSnIB3s2",
        "outputId": "eb947820-6325-4801-9217-fd67f298b11b"
      },
      "source": [
        "# Print classification report.\n",
        "y_pred = decision_tree.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test,y_pred)\n",
        "decision_tree_report = classification_report(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy: \", dt_accuracy)\n",
        "print(\"Classification report:\")\n",
        "print(decision_tree_report)\n",
        "print(\"Confusion matrix:\")\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjFat3rlSase"
      },
      "source": [
        "This decision tree model with PCA decomposed features is performing better now with a lower run time. However, in comparison to our decision tree classifier using SelectKBest features, model run time is about 10x longer. Accuracy score has only improved from .956 with SelectKBest to .982 with PCA decomposed features. So the decision tree has a trade off between processing time, dimensionality, and accuracy that must be established for each use case. It seems hard to justify the increase in run time for the small benefit of slight increase in accuracy scoring seen here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNyef6XVCI9L"
      },
      "source": [
        "## Random Forest Classifier with GridSearchCV and PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CMysdg7CI9M",
        "outputId": "41420046-ebd3-4977-b3ae-7c55535d5678"
      },
      "source": [
        "# Timing our model.\n",
        "start_time = time.time()\n",
        "\n",
        "# Building random forest classifier.\n",
        "rfc = ensemble.RandomForestClassifier()\n",
        "\n",
        "# Fitting classifier to training data. \n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "# Print model runtime.\n",
        "print(\"--- %s seconds ---\" % round(time.time() - start_time, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwH08yxrUJCp"
      },
      "source": [
        "Creating and timing our initial model before processing with GridSearchCV to find the best hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6qkWOZjCI9R",
        "outputId": "897dc17b-c7ab-4df8-fb84-65d9eee82cbb"
      },
      "source": [
        "# Creating hyperparameter grid.\n",
        "param_grid = { \n",
        "    'n_estimators': [5,50,100,500],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'max_depth' : [4,6,8],\n",
        "    'criterion' :['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Setting up GridSearchCV to run on our random forest classifier.\n",
        "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 3)\n",
        "\n",
        "# Fitting GridSearchCV to training data. \n",
        "CV_rfc.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJaYIINMCI9T",
        "outputId": "4c3032cb-d47c-4519-dab5-8d17ff28b86b"
      },
      "source": [
        "# Calling .best_params_ to pull best parameters from GridSearchCV.\n",
        "CV_rfc.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E3kPWCXCI9V",
        "outputId": "b700fce3-02d2-4168-a79a-dde7293b9cae"
      },
      "source": [
        "# Timing our model.\n",
        "start_time = time.time()\n",
        "\n",
        "# Building random forest classifier with best parameters from GridSearchCV.\n",
        "rfc = ensemble.RandomForestClassifier(criterion='gini', \n",
        "                                      max_depth=8,\n",
        "                                      max_features='sqrt',\n",
        "                                      n_estimators=100, random_state=42)\n",
        "\n",
        "# Fitting RFC with new hyperparameters.\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "# Printing model runtime.\n",
        "print(\"--- %s seconds ---\" % round(time.time() - start_time, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsNUnm-9VB2k"
      },
      "source": [
        "Using optimized hyperparameters from GridSearchCV we were able to reduce run time by about 40%. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2FY6SblCI9X",
        "outputId": "978bc1f4-1fd5-446a-fa4e-17fc6fe381f3"
      },
      "source": [
        "# Print cross-validation score.\n",
        "cross_val_score(rfc, X_train, y_train, cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXRKinhhJA01"
      },
      "source": [
        "Cross-validation score is consistent and does not show any signs of overfitting or underfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m9JBHlCCI9Z",
        "outputId": "a8c7dedb-0f45-4c34-c55e-d13221cd37e8"
      },
      "source": [
        "# Print classification report.\n",
        "y_pred = rfc.predict(X_test) \n",
        "rfc_accuracy = accuracy_score(y_test,y_pred)\n",
        "rfc_report = classification_report(y_test, y_pred)\n",
        "rfc_cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Classification report:\")\n",
        "print(\"Accuracy: \", rfc_accuracy)\n",
        "print(rfc_report)\n",
        "print(\"Confusion matrix:\")\n",
        "print(rfc_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsUS1d7KJFBA"
      },
      "source": [
        "The previous decision tree model with PCA decomposed features is performing better than the random forest model here in terms of both runtime and scoring. Random forest run time is longer with a reduction in accuracy score and weighted averages for precision, recall, and f1-score.  As shown again here ensemble models are not always better performers than their individual counterparts, they are better if the single decision tree models suffer from instability most usually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5RaELcwc3iZ"
      },
      "source": [
        "In conclusion, our KNN classifier worked best using PCA feature reduced data with the best combination of run time and performance. Decision tree performed best in terms of scoring but at the cost of a long run time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGjen1dj8vdT"
      },
      "source": [
        "# Unsupervised Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naFq_5-MzPHU"
      },
      "source": [
        "We will use KMeans and DBSCAN clustering techniques on our data using PCA and t-SNE feature reduced data to see which combination of dimensionality reduction and clustering techniques works best for this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWMfb2hFPSjB"
      },
      "source": [
        "## PCA Feature Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCqzgfMFugdL"
      },
      "source": [
        "# df loaded above (re-run first load cell if needed)\n",
        "df.head(2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR-sL6pRVuwI"
      },
      "source": [
        "# Dropping redundant columns 'to_id' and 'from_id' from DataFrame.\n",
        "df = df.drop(columns='to_id')\n",
        "df = df.drop(columns='from_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1szWoLFVvjZ"
      },
      "source": [
        "Dropping redundant features such as to_id and from_id since our actual locations are listed in features to and from and can be used as dummy variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNhtnkBDV0dL"
      },
      "source": [
        "# Dropping missing values from DataFrame.\n",
        "df = df.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC-gK_P9WIM1"
      },
      "source": [
        "# Drop redundant columns once again from DataFrame.\n",
        "df = df.drop(columns='scheduled_time')\n",
        "df = df.drop(columns='actual_time')\n",
        "df = df.drop(columns='date')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvX9Y3-UWIhV"
      },
      "source": [
        "Dropping the scheduled time and actual time columns since we are only interested in the amount of delay which is measured and already extracted from these columns for us in the 'delay_minutes' feature. The date feature is also not important to our clustering model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GJtFIhRWRD4"
      },
      "source": [
        "# Dropping 'type' column, contains only one value.\n",
        "df = df.drop(columns='type')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnyE22fxWRiZ"
      },
      "source": [
        "We can drop our type feature as after dropping missing values it only contains one unique value and won't contribute to our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTRCfKuuWcXW"
      },
      "source": [
        "# Creating a new feature to indicate if a train is late or not, we will use a threshold of 15 minute delay time.\n",
        "df['late'] = df['delay_minutes']>15\n",
        "\n",
        "# We can now drop our delay column as we have extracted the necessary info from it.\n",
        "df = df.drop(columns='delay_minutes')\n",
        "\n",
        "# One-hot encoding statistical variables for machine learning.\n",
        "df = pd.get_dummies(df, columns={'from','to','line','train_id','status','late'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2AfpREwWC_I"
      },
      "source": [
        "Before Principle Component Analysis, we must scale our features to bring the mean to zero and the standard deviation to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CojLuT5jLt_0",
        "outputId": "df505f3b-daa1-4b22-8a8b-9d1f1e1969a3"
      },
      "source": [
        "# Standardize features by removing the mean and scaling to unit variance with sklearns StandardScaler.\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit scaler to data by calling fit_transform on our data.\n",
        "X_scale = scaler.fit_transform(df)\n",
        "\n",
        "# Storing scaled data into new DataFrame.\n",
        "df_scale = pd.DataFrame(X_scale, columns=df.columns)\n",
        "df_scale.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSkpTVku0Bm0"
      },
      "source": [
        "# PCA with the number of components set to 2, this will reduce our data to two dimensions.\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# We get the components by calling fit_transform method with our data.\n",
        "pca_components = pca.fit_transform(df_scale)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFxw-5E30dP1",
        "outputId": "1ac9adb1-ffc6-4912-82e7-efb47f741c12"
      },
      "source": [
        "# Defining pyplot figure size.\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "# Creating scatterplot of pca component dimensions with pyplot.\n",
        "plt.scatter(pca_components[:, 0], pca_components[:, 1])\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI3XXf0XXnND"
      },
      "source": [
        "This scatterplot shows what our data looks like now reduced to two dimensions, KMeans and DBSCAN will label these clusters according to their respective algorithms and we will test the performance of each using the silhouette score metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZIyc7vsPfS9"
      },
      "source": [
        "## Applying KMeans to PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "dYx9wwscPis8",
        "outputId": "91046b50-c02d-404b-f1b6-afb0a0ef8642"
      },
      "source": [
        "# Components are now in Pandas DataFrame.\n",
        "PCA_components = pd.DataFrame(pca_components)\n",
        "# Create range of k values.\n",
        "ks = range(1, 10)\n",
        "inertias = []\n",
        "for k in ks:\n",
        "    # Create KMeans instance with k clusters: model.\n",
        "    model = KMeans(n_clusters=k)\n",
        "\n",
        "    # Fit model to samples.\n",
        "    model.fit(PCA_components.iloc[:,:3])\n",
        "    \n",
        "    # Append the inertia to the list of inertias.\n",
        "    inertias.append(model.inertia_)\n",
        "    \n",
        "plt.plot(ks, inertias, '-o', color='black')\n",
        "plt.xlabel('number of clusters, k')\n",
        "plt.ylabel('inertia')\n",
        "plt.xticks(ks)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXBUWa9DYMVY"
      },
      "source": [
        "The elbow method uses the K value plotted on the x-axis and the inertia plotted on the y-axis to choose where the plot comes to a point or 'elbow'. This visual method may give us an area to focus on but it is wise to go ahead and score all clusters up to 10 statistically. The elbow method here would give us a KMeans cluster value of 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_PUpeUzPj1y",
        "outputId": "a4976d61-29fe-4fea-b6be-fba484804507"
      },
      "source": [
        "# Defining KMeans cluster number of 2 and labeling PCA components.\n",
        "labels = KMeans(n_clusters=2, random_state=42).fit_predict(PCA_components)\n",
        "# Print silhouette score of KMeans.\n",
        "print(metrics.silhouette_score(PCA_components, labels, metric='euclidean'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_E6cGc2naFgJ",
        "outputId": "15213552-5cba-4d4a-90d5-1b76aead2a80"
      },
      "source": [
        "labels = KMeans(n_clusters=3, random_state=42).fit_predict(PCA_components)\n",
        "print(metrics.silhouette_score(PCA_components, labels, metric='euclidean'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG-bK532Ze_i"
      },
      "source": [
        "Elbow method score, let's see if this score remains the highest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zh7gzm4yaGJz",
        "outputId": "4ec17d97-0ac1-40d3-a6ff-1911b143b08f"
      },
      "source": [
        "labels = KMeans(n_clusters=4, random_state=42).fit_predict(PCA_components)\n",
        "print(metrics.silhouette_score(PCA_components, labels, metric='euclidean'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-AD8H7GZHEI"
      },
      "source": [
        "The silhouette score seen here is very close to our elbow value for KMeans of 3 but still an improvement. If we only relied on visual methods we may have chose a KMeans value that did not perform as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvqA7x3OaIRF",
        "outputId": "b7bc454c-a7a0-4a18-b97c-c791c7ae50b5"
      },
      "source": [
        "labels = KMeans(n_clusters=5, random_state=42).fit_predict(PCA_components)\n",
        "print(metrics.silhouette_score(PCA_components, labels, metric='euclidean'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eRqj7hqaJip",
        "outputId": "a3f68558-a79a-46e1-e79d-4e82cc0e032b"
      },
      "source": [
        "labels = KMeans(n_clusters=6, random_state=42).fit_predict(PCA_components)\n",
        "print(metrics.silhouette_score(PCA_components, labels, metric='euclidean'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtE60_3XaK3k",
        "outputId": "c0a2bffb-592a-4a10-c1fa-18c3b72effa2"
      },
      "source": [
        "labels = KMeans(n_clusters=7, random_state=42).fit_predict(PCA_components)\n",
        "print(metrics.silhouette_score(PCA_components, labels, metric='euclidean'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DW8R2W_laMe-",
        "outputId": "71e00d4d-5e58-4c42-a9d5-4edb3e7275f9"
      },
      "source": [
        "labels = KMeans(n_clusters=8, random_state=42).fit_predict(PCA_components)\n",
        "print(metrics.silhouette_score(PCA_components, labels, metric='euclidean'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlQFYZqYaOvx",
        "outputId": "df5a9221-50ed-41f8-ed51-8513e83a079a"
      },
      "source": [
        "labels = KMeans(n_clusters=9, random_state=42).fit_predict(PCA_components)\n",
        "print(metrics.silhouette_score(PCA_components, labels, metric='euclidean'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CRiP8aGaRL2",
        "outputId": "ded9a3ce-9785-4a6c-fd3f-cf7ed5242d17"
      },
      "source": [
        "labels = KMeans(n_clusters=10, random_state=42).fit_predict(PCA_components)\n",
        "print(metrics.silhouette_score(PCA_components, labels, metric='euclidean'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38tHJXO9PxJ_",
        "outputId": "c282e3da-b098-493b-9426-57c235c889b9"
      },
      "source": [
        "# Defining the k-means with the best silhouette value (4) and timing it.\n",
        "kmeans_cluster = KMeans(n_clusters=4, random_state=42)\n",
        "# Fitting and timing clustering.\n",
        "%timeit kmeans_cluster.fit(PCA_components)\n",
        "y_pred = kmeans_cluster.predict(PCA_components)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul8MHQSnFPGN"
      },
      "source": [
        "Timing and building the best performing model with a KMeans value of 4 and a silhouette score of .8. A perfect silhouette score would be a value of 1. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxkEXHZQ9CO9"
      },
      "source": [
        "## Cluster Analysis of KMeans Clusters using PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "238F4hQZGJrf"
      },
      "source": [
        "# df loaded above (re-run first load cell if needed)\n",
        "df.head(2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTVRiceBRi1L"
      },
      "source": [
        "Now that we have recreated our DataFrame before processing we can create a feature to label the data to its respective cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIpcEvSAG--U"
      },
      "source": [
        "# Finally, attach the cluster labels from KMeans to the original DataFrame.\n",
        "df2['cluster_label'] = kmeans_cluster.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "5BHGh-1S8-Fs",
        "outputId": "15c2cddd-8fbc-4e55-e3fd-7cf5c1e585c8"
      },
      "source": [
        "# Examining DataFrame to confirm cluster labels have been attached.\n",
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snMJPI1aHjbT"
      },
      "source": [
        "# Creating boolean values to create new cluster DataFrames.\n",
        "clusters1 = df2['cluster_label'] == 0\n",
        "clusters2 = df2['cluster_label'] == 1\n",
        "clusters3 = df2['cluster_label'] == 2\n",
        "clusters4 = df2['cluster_label'] == 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1NzUBbNHwFJ"
      },
      "source": [
        "# Applying boolean against dataframe to pull correct rows.\n",
        "clusters1df = df2[clusters1]\n",
        "clusters2df = df2[clusters2]\n",
        "clusters3df = df2[clusters3]\n",
        "clusters4df = df2[clusters4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di1WHSmESGh4"
      },
      "source": [
        "Now that we have labeled our data with the correct cluster and put the clusters into an individual DataFrame, we can analyze and plot each cluster by analyzing the cluster DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6cNi7nyNC_r"
      },
      "source": [
        "### First Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIVza-XnYdp7",
        "outputId": "f7436f97-6bb4-4ddc-c01b-35f671c7b6dc"
      },
      "source": [
        "# Examining cluster size.\n",
        "clusters1df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X4Qgb6kYgNx"
      },
      "source": [
        "The first cluster is the largest in comparison to the other four clusters with our PCA data using KMeans clustering. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "q-LjE8wYLwOK",
        "outputId": "78e16952-e7c8-49ce-bf5e-958ff34bff53"
      },
      "source": [
        "# Creating countplot with seaborn and defining figure size.\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(clusters1df['late'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6svT_h-YziJ"
      },
      "source": [
        "This cluster is comprised of mostly unlate trains with some late trains slipping through as shown here on the countplot for our late variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "JYfGQrlKVIjK",
        "outputId": "d89e3b1c-8535-4ab8-c12d-c085327aa2d4"
      },
      "source": [
        "# Defining figure size.\n",
        "plt.figure(figsize=(10,6))\n",
        "# Creating countplot of train lines for analysis using seaborn.\n",
        "sns.countplot(clusters1df['line'], hue=clusters1df['late'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuGtK75cUQra"
      },
      "source": [
        "This cluster seems to be representative of mostly unlate trains from various lines. We see some late values for some train lines in here but with counts significantly less than unlate trains. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFcUXVXJNPI9"
      },
      "source": [
        "### Second Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvckOjybYQGP",
        "outputId": "998cda10-35a6-4d5a-b479-b4e410d20a3d"
      },
      "source": [
        "# Examining cluster size.\n",
        "clusters2df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x72gHVR1Zr7Q"
      },
      "source": [
        "This is the second largest cluster in size compared to other four clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "2TVxG5AQWT8f",
        "outputId": "a445e7f5-d697-4dae-a147-b641a32e0460"
      },
      "source": [
        "# Creating countplot of our late variable with seaborn and defining figure size.\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(clusters2df['late'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oNs5f58ZyhB"
      },
      "source": [
        "This second cluster is also comprised of mostly unlate trains with a few late values slipping in like before in the first cluster. This is not surprising considering the size of this cluster as it is almost half the dataset like the first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "-B14Rt1SNTpz",
        "outputId": "1506a51d-8a70-4ebc-d0e8-fdd902c705f5"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(clusters2df['line'], hue=clusters2df['late'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6KQhv4RbUoB"
      },
      "source": [
        "This cluster is comprised mostly of different train lines than the first. We have some very small amounts of some train lines from the first cluster like the Morristown line. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2PX8GQfNbmu"
      },
      "source": [
        "### Third Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEAfM4suX_Os",
        "outputId": "631d9a84-0236-432c-d717-fd1deb19f329"
      },
      "source": [
        "# Examining cluster size.\n",
        "clusters3df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7QP-QMUbgbd"
      },
      "source": [
        "This cluster size is much smaller than our first two clusters with only 1,124 values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "8TsBdy5ZWWrK",
        "outputId": "a2a4ecb9-119f-4808-fca6-095895211c74"
      },
      "source": [
        "# Creating countplot of our late variable with seaborn and defining figure size.\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(clusters3df['late'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTRh2mH0chHK"
      },
      "source": [
        "Unlike the first two clusters also this cluster is entirely comprised of late trains with no false values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "S9l58yaBNYd1",
        "outputId": "eb9351c0-ba54-404a-a58a-3e416677d1f6"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "# Creating countplot of train lines for analysis using seaborn.\n",
        "sns.countplot(clusters3df['line'], hue=clusters3df['late'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRIxrWDtc1qS"
      },
      "source": [
        "Clustering algorith grouped only trains from one line into this cluster interestingly. This suggests the Atlantic City Line is in need of upgrades."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEt5tuKBNpis"
      },
      "source": [
        "### Fourth Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiAt1RpGYFMt",
        "outputId": "18db2ce9-883c-4d8a-e2eb-2962d57235bc"
      },
      "source": [
        "# Examining cluster size.\n",
        "clusters4df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "Uj9DqWBQWd8f",
        "outputId": "e5b4843f-ae97-4260-bb1a-5dee1a97307a"
      },
      "source": [
        "# Creating countplot of our late variable with seaborn and defining figure size.\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(clusters4df['late'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "RngY12peNsQk",
        "outputId": "46a486b3-b153-47b6-f559-6a8f5bb202e3"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "# Creating countplot of train lines for analysis using seaborn.\n",
        "sns.countplot(clusters4df['line'], hue=clusters4df['late'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN4xtqUEPLwX"
      },
      "source": [
        "It seems that the Montclair-Boonton and Pascack Valley lines are in need of upgrades also. Our clustering algorithm was able to group these lines from the other clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "Qu7XJI9QdNaI",
        "outputId": "4115746e-3115-4984-b30a-f1afa6872b70"
      },
      "source": [
        "# Finally, let's visualize the clusters formed by KMeans and PCA with a simple scatterplot.\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.scatter(PCA_components[0],PCA_components[1], c=y_pred); \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYKQq4eDcH0O"
      },
      "source": [
        "## Applying DBSCAN Clustering to PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tz0qdqwVcPKY",
        "outputId": "8f2f823b-b8e3-427a-ebdd-a79ad69d0b5b"
      },
      "source": [
        "dbscan_cluster = DBSCAN(eps=.2, min_samples=150)\n",
        "labels = dbscan_cluster.fit_predict(pca_components)\n",
        "print(\"The silhouette score of the DBSCAN solution: {}\"\n",
        "      .format(metrics.silhouette_score(pca_components, labels, metric='euclidean')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtpBWLQSH_Gm"
      },
      "source": [
        "Defining the density based algorithm DBSCAN and adjusting the epsilon and minimum sample values. The silhouette score may be improved upon with different hyperparameter values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctRPQWFc1JjK",
        "outputId": "d9318cf5-2e73-4ac9-f45e-ad15a82cec7c"
      },
      "source": [
        "dbscan_cluster = DBSCAN(eps=.1, min_samples=150)\n",
        "labels = dbscan_cluster.fit_predict(pca_components)\n",
        "print(\"The silhouette score of the DBSCAN solution: {}\"\n",
        "      .format(metrics.silhouette_score(pca_components, labels, metric='euclidean')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbJ2XHfIIoQp"
      },
      "source": [
        "Reducing the epsilon value seems to have made our model perform slightly better, let's try increasing the minimum sample value instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSVqBc__8z5I",
        "outputId": "3cb0c615-7c60-4eed-865b-5ab7c88c5d47"
      },
      "source": [
        "dbscan_cluster = DBSCAN(eps=.2, min_samples=200)\n",
        "labels = dbscan_cluster.fit_predict(pca_components)\n",
        "print(\"The silhouette score of the DBSCAN solution: {}\"\n",
        "      .format(metrics.silhouette_score(pca_components, labels, metric='euclidean')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM2NTbVzJFeX"
      },
      "source": [
        "Slight improvement on score with increased minimum sample value and using the initial epsilon value. Let's try to increase the epsilon value further."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcKH3-hD6Wye"
      },
      "source": [
        "dbscan_cluster = DBSCAN(eps=.3, min_samples=200)\n",
        "labels = dbscan_cluster.fit_predict(pca_components)\n",
        "print(\"The silhouette score of the DBSCAN solution: {}\"\n",
        "      .format(metrics.silhouette_score(pca_components, labels, metric='euclidean')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIAgMHOg_OzK"
      },
      "source": [
        "Setting the EPS at .3 causes our kernel to crash with a large minimum sample value, so we will try further adjusting our minimum sample value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMaOnTra-cOn",
        "outputId": "4ceee8ff-55bd-4066-8fd1-1e9029dfd1bb"
      },
      "source": [
        "dbscan_cluster = DBSCAN(eps=.1, min_samples=250)\n",
        "labels = dbscan_cluster.fit_predict(pca_components)\n",
        "print(\"The silhouette score of the DBSCAN solution: {}\"\n",
        "      .format(metrics.silhouette_score(pca_components, labels, metric='euclidean')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3ydowm4gX5u"
      },
      "source": [
        "Increased minimum sample size caused reduced performance for DBSCAN clustering here. Let's try a smaller sample size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPCcyiSXgehB",
        "outputId": "6807f677-af86-4a90-f2fa-b823fb99c870"
      },
      "source": [
        "dbscan_cluster = DBSCAN(eps=.1, min_samples=25)\n",
        "labels = dbscan_cluster.fit_predict(pca_components)\n",
        "print(\"The silhouette score of the DBSCAN solution: {}\"\n",
        "      .format(metrics.silhouette_score(pca_components, labels, metric='euclidean')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssvYU_wMhesZ",
        "outputId": "46dd1a45-392f-4d91-82b9-576ce84a197e"
      },
      "source": [
        "dbscan_cluster = DBSCAN(eps=.1, min_samples=100)\n",
        "labels = dbscan_cluster.fit_predict(pca_components)\n",
        "print(\"The silhouette score of the DBSCAN solution: {}\"\n",
        "      .format(metrics.silhouette_score(pca_components, labels, metric='euclidean')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NFtEj29iF_E",
        "outputId": "82fe6702-b892-4e92-b34a-ba219575ce8f"
      },
      "source": [
        "dbscan_cluster = DBSCAN(eps=.2, min_samples=100)\n",
        "labels = dbscan_cluster.fit_predict(pca_components)\n",
        "print(\"The silhouette score of the DBSCAN solution: {}\"\n",
        "      .format(metrics.silhouette_score(pca_components, labels, metric='euclidean')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkNdOfxUjQJO",
        "outputId": "747f20a9-fcb4-4b76-bca5-0a6bfd35018b"
      },
      "source": [
        "dbscan_cluster = DBSCAN(eps=.3, min_samples=125)\n",
        "labels = dbscan_cluster.fit_predict(pca_components)\n",
        "print(\"The silhouette score of the DBSCAN solution: {}\"\n",
        "      .format(metrics.silhouette_score(pca_components, labels, metric='euclidean')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_a0MCJB9EEa"
      },
      "source": [
        "Hyperparameter tuning was able to increase model performance significantly adjusting minimum sample value and finally settling on a value of 125 and epsilon value of .3. Although silhouette score performance is not better than KMeans it may still provide meaningful clustering so we will analyze the clusters below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xx8dwyudqGj"
      },
      "source": [
        "## DBSCAN Cluster Analysis with PCA Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iphDPRsmowT",
        "outputId": "56bfe995-e2c4-4aa7-b3d3-3ee8610456c8"
      },
      "source": [
        "np.unique(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKqFYp-om3cF"
      },
      "source": [
        "Let's see how many clusters our DBSCAN algorithm was able to identify by calling unique values for labeled clustering, we have 9 clusters. The -1 value is where DBSCAN grouped outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4OJRIknddjb"
      },
      "source": [
        "df2['dbscan_pca_cluster_label'] = labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1KiSGudk_AB"
      },
      "source": [
        "Attaching DBSCAN cluster labels to original dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuNnJq6QlC0P"
      },
      "source": [
        "cluster1 = df2['dbscan_pca_cluster_label'] == 0\n",
        "cluster2 = df2['dbscan_pca_cluster_label'] == 1\n",
        "cluster3 = df2['dbscan_pca_cluster_label'] == 2\n",
        "cluster4 = df2['dbscan_pca_cluster_label'] == 3\n",
        "cluster5 = df2['dbscan_pca_cluster_label'] == 4\n",
        "cluster6 = df2['dbscan_pca_cluster_label'] == 5\n",
        "cluster7 = df2['dbscan_pca_cluster_label'] == 6\n",
        "cluster8 = df2['dbscan_pca_cluster_label'] == 7\n",
        "cluster9 = df2['dbscan_pca_cluster_label'] == 8\n",
        "outliercluster = df2['dbscan_pca_cluster_label'] == -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGxiF6NblX7C"
      },
      "source": [
        "Creating boolean values to create new cluster dataframes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQA--eSDldnB"
      },
      "source": [
        "cluster1df = df2[cluster1]\n",
        "cluster2df = df2[cluster2]\n",
        "cluster3df = df2[cluster3]\n",
        "cluster4df = df2[cluster4]\n",
        "cluster5df = df2[cluster5]\n",
        "cluster6df = df2[cluster6]\n",
        "cluster7df = df2[cluster7]\n",
        "cluster8df = df2[cluster8]\n",
        "cluster9df = df2[cluster9]\n",
        "outlierclusterdf = df2[outliercluster]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlnIvXg8liTb"
      },
      "source": [
        "Applying boolean against dataframe to pull correct rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sesrzplNlxDl"
      },
      "source": [
        "### First Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "a7ISj3ZBltXh",
        "outputId": "5b2ad680-167b-4c8c-8349-fe7c80a52694"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(cluster1df['line'], hue=cluster1df['late'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTQweskkl3WT"
      },
      "source": [
        "### Second Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "ygtgySdml7ys",
        "outputId": "f0bbdefb-fd6e-4a62-b72d-18ade8641474"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(cluster2df['line'], hue=cluster2df['late'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnmkTkxamBja"
      },
      "source": [
        "### Third Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "mA-_-Za3mEPi",
        "outputId": "2fc1177f-bb0f-48b3-8847-ba797e48fa4a"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(cluster3df['line'], hue=cluster3df['late'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYYXQH3YmH4r"
      },
      "source": [
        "### Fourth Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "PX550wbymKHi",
        "outputId": "f94a2ae7-6de8-47ba-c950-648c12c20998"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(cluster4df['line'], hue=cluster4df['late'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-QwT6K9mW8l"
      },
      "source": [
        "DBSCAN clustering algorithm was able to correctly group only late trains into the fourth cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xw8usxunj4E"
      },
      "source": [
        "### Fifth Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "bCpzkia1mhbL",
        "outputId": "5416fed8-74be-4895-9684-f1948ee0eaf3"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(cluster5df['line'], hue=cluster5df['late'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJCnPX05nqP2"
      },
      "source": [
        "### Sixth Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "7r9Fl2p0ntVt",
        "outputId": "bddab7e6-de88-4f0f-beac-17bbf65ff00a"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(cluster6df['line'], hue=cluster6df['late'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KlLuMmxnxo3"
      },
      "source": [
        "DBSCAN was able to correctly group late trains from the Atl. City line in this cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPm3w0yjn2-e"
      },
      "source": [
        "### Seventh Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "KuVhyxT-n5dA",
        "outputId": "a1ac161f-25e2-489f-ac49-7aea88542fc2"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(cluster7df['line'], hue=cluster7df['late'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of1GRcj5n9M3"
      },
      "source": [
        "DBSCAN correctly grouped late trains again this time from the Pascack Valley and Montclair-Boonton lines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIinO-O2oGO2"
      },
      "source": [
        "### Eigth Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "SI127tp4oI_Q",
        "outputId": "20333630-4090-494d-e036-76f37e928856"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(cluster8df['line'], hue=cluster8df['late'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzG5RYdfhr_A"
      },
      "source": [
        "This cluster is also exclusively grouped with a true late status."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdEwB2-fhkrb"
      },
      "source": [
        "### Ninth Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "R1f_UZJnhlJe",
        "outputId": "f8ddfa3e-6874-4836-ad26-ee78a5ca0284"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(cluster9df['line'], hue=cluster9df['late'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFsWxR4doNYO"
      },
      "source": [
        "DBSCAN again grouped trains that were late into this cluster. In comparison to KMeans while the silhouette performance is reduced, the DBSCAN algorithm seems to be grouping trains better in regards to late status. While KMeans was grouping lines with both late and not late trains, DBSCAN seems to be grouping exclusively late or not late trains into certain clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "cV5UJOkTeJYy",
        "outputId": "c23a8419-8969-4d0b-f39f-3d5ebb8d6487"
      },
      "source": [
        "# Visualizing DBSCAN clustering.\n",
        "plt.figure(figsize=(11,7))\n",
        "labels = dbscan_cluster.fit_predict(pca_components)\n",
        "plt.scatter(PCA_components[0],PCA_components[1], c=labels); \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edRwVUBlzyll"
      },
      "source": [
        "# t-SNE Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaT1mZdg0lCs",
        "outputId": "ea043b6b-4532-4cc0-cd32-62df114d34c2"
      },
      "source": [
        "time_start = time.time()\n",
        "tsne = TSNE(n_components=2, verbose=1, perplexity=90, max_iter=1000)\n",
        "tsne_results = tsne.fit_transform(pca_components)\n",
        "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu3TYghNeJa7"
      },
      "source": [
        "Running and timing t-SNE. t-Distributed Stochastic Neighbor Embedding (t-SNE) is a newer method of dimensionality reduction like PCA. Unlike PCA it tries to minimize the KullbackLeibler divergence (KL divergence) between the two dimensions which can allow for better visualization of high-dimensional data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJEWT6y1QU63"
      },
      "source": [
        "### Applying KMeans to t-SNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "ryPinJvrQFtj",
        "outputId": "b3899d47-d07d-4ac1-9e82-e5cea4e3b019"
      },
      "source": [
        "# t-SNE reduced components stored in Pandas DataFrame.\n",
        "tsne_components = pd.DataFrame(tsne_results)\n",
        "# Defining range of k-values.\n",
        "ks = range(1, 10)\n",
        "inertias = []\n",
        "for k in ks:\n",
        "    # Create KMeans instance with k clusters: model.\n",
        "    model = KMeans(n_clusters=k)\n",
        "    # Fit model to samples.\n",
        "    model.fit(tsne_components.iloc[:,:3])\n",
        "    # Append the inertia to the list of inertias.\n",
        "    inertias.append(model.inertia_)\n",
        "    \n",
        "plt.plot(ks, inertias, '-o', color='black')\n",
        "plt.xlabel('number of clusters, k')\n",
        "plt.ylabel('inertia')\n",
        "plt.xticks(ks)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9pVIUmwd45a"
      },
      "source": [
        "Let's find examine our elbow to get a feel for the best k-value before we confirm it statistically with our silhouette score. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irAjN07DQGRb",
        "outputId": "3e3b4d4c-fd2c-4683-f211-1902054ce88b"
      },
      "source": [
        "labels = KMeans(n_clusters=2, random_state=42).fit_predict(tsne_results)\n",
        "print(metrics.silhouette_score(tsne_results, labels, metric='euclidean'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKr8fzE3aYN3",
        "outputId": "551428b9-fe96-4251-c974-10b4d78aa876"
      },
      "source": [
        "labels = KMeans(n_clusters=3, random_state=42).fit_predict(tsne_results)\n",
        "print(metrics.silhouette_score(tsne_results, labels, metric='euclidean'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoYYERjwt59s"
      },
      "source": [
        "Elbow method proves ineffective here as we have the highest silhouette score with a KMeans value of 3. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Pb9JYUZaYwg",
        "outputId": "7dfeb563-6d16-43e7-ac41-070272787971"
      },
      "source": [
        "labels = KMeans(n_clusters=4, random_state=42).fit_predict(tsne_results)\n",
        "print(metrics.silhouette_score(tsne_results, labels, metric='euclidean'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pufcQIXiaaSD",
        "outputId": "5ac2eb33-702f-4506-c72b-d055fe4f1d4b"
      },
      "source": [
        "labels = KMeans(n_clusters=5, random_state=42).fit_predict(tsne_results)\n",
        "print(metrics.silhouette_score(tsne_results, labels, metric='euclidean'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4axUKPrabg_",
        "outputId": "8a13454b-33f2-4e73-8b09-ef3740d093e0"
      },
      "source": [
        "labels = KMeans(n_clusters=6, random_state=42).fit_predict(tsne_results)\n",
        "print(metrics.silhouette_score(tsne_results, labels, metric='euclidean'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGhZ4WPPadXS",
        "outputId": "38577b9c-c3d9-42d3-92d5-ea3c188155f7"
      },
      "source": [
        "labels = KMeans(n_clusters=7, random_state=42).fit_predict(tsne_results)\n",
        "print(metrics.silhouette_score(tsne_results, labels, metric='euclidean'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXNKNnyCaeyM",
        "outputId": "852793d7-0212-4781-a82a-9521233ad2ad"
      },
      "source": [
        "labels = KMeans(n_clusters=8, random_state=42).fit_predict(tsne_results)\n",
        "print(metrics.silhouette_score(tsne_results, labels, metric='euclidean'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX57vQF5agSq",
        "outputId": "2fc6172a-6642-494c-92f9-df749a6752e0"
      },
      "source": [
        "labels = KMeans(n_clusters=9, random_state=42).fit_predict(tsne_results)\n",
        "print(metrics.silhouette_score(tsne_results, labels, metric='euclidean'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D1SRk0rahnn",
        "outputId": "93a007b6-50b6-4fa0-c7aa-7a53755d160e"
      },
      "source": [
        "labels = KMeans(n_clusters=10, random_state=42).fit_predict(tsne_results)\n",
        "print(metrics.silhouette_score(tsne_results, labels, metric='euclidean'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djfl21eNQLXN",
        "outputId": "b59edaf0-36bf-46d1-efeb-596e26bb43dd"
      },
      "source": [
        "# Defining the k-means with the best silhouette value (3) and timing it.\n",
        "kmeans_cluster = KMeans(n_clusters=3, random_state=42)\n",
        "\n",
        "# Fit model.\n",
        "%timeit kmeans_cluster.fit(tsne_results)\n",
        "y_pred = kmeans_cluster.predict(tsne_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb1x7hfauKlY"
      },
      "source": [
        "## KMeans Cluster Analysis with t-SNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl58ClCwuRBC"
      },
      "source": [
        "# Attaching cluster labels to original dataframe.\n",
        "df2['kmeans_tsne_cluster_label'] = kmeans_cluster.labels_\n",
        "\n",
        "# Creating boolean arrays to pull clusters from original dataset.\n",
        "cluster1 = df2['kmeans_tsne_cluster_label'] == 0\n",
        "cluster2 = df2['kmeans_tsne_cluster_label'] == 1\n",
        "cluster3 = df2['kmeans_tsne_cluster_label'] == 2\n",
        "\n",
        "# Creating dataframe for each cluster found with KMeans and t-SNE.\n",
        "cluster1df = df2[cluster1]\n",
        "cluster2df = df2[cluster2]\n",
        "cluster3df = df2[cluster3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm1cDEHVQXq0"
      },
      "source": [
        "### First Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "G65RFnDcQass",
        "outputId": "1c2c3d60-5f17-494f-f592-157bea12b15e"
      },
      "source": [
        "plt.figure(figsize=(17,6))\n",
        "sns.countplot(cluster1df['line'], hue=cluster1df['late'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT4tnt5vQpA6"
      },
      "source": [
        "### Second Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "om9wrDSkQpi1",
        "outputId": "06239dd5-70e9-4764-ee98-120f99ed2c18"
      },
      "source": [
        "plt.figure(figsize=(17,6))\n",
        "sns.countplot(cluster2df['line'], hue=cluster2df['late'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abJJcgOwQp-N"
      },
      "source": [
        "### Third Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "V043le6dQqXR",
        "outputId": "e026aaec-01df-4fd0-9dee-bf2cf2097064"
      },
      "source": [
        "plt.figure(figsize=(17,6))\n",
        "sns.countplot(cluster3df ['line'], hue=cluster3df ['late'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "z_Pf5Tqb5EqH",
        "outputId": "440946c2-a5ef-4663-b328-504416bca910"
      },
      "source": [
        "# Finally, let's visualize the clusters formed by KMeans and t-SNE with a simple scatterplot.\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.scatter(tsne_components[0],tsne_components[1], c=y_pred); \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzqrxuB_Rq91"
      },
      "source": [
        "The clustering seen here is very simple and does not conform to the reduced dimensional shape all too much. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTwtNEWNd7TV"
      },
      "source": [
        "## Applying DBSCAN Clustering to t-SNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQwljUAAeE9i",
        "outputId": "6fcd590b-6290-4c85-ef7d-2f1d7d29f11f"
      },
      "source": [
        "dbscan_cluster = DBSCAN(eps=.1, min_samples=50)\n",
        "clusters = dbscan_cluster.fit_predict(tsne_results)\n",
        "print(\"The silhouette score of the DBSCAN solution: {}\"\n",
        "      .format(metrics.silhouette_score(tsne_results, clusters, metric='euclidean')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5Wg7NQbuaSr"
      },
      "source": [
        "Defining the density based algorithm DBSCAN and adjusting the epsilon and minimum sample values. The silhouette score may hopefully be improved upon with different hyperparameter values as our DBSCAN clustering algorithm is performing significantly worse on t-SNE reduced features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCX_88K4eLOg",
        "outputId": "2eca00ed-2a55-4186-c0cb-bf9c9b52f903"
      },
      "source": [
        "dbscan_cluster = DBSCAN(eps=.1, min_samples=75)\n",
        "clusters = dbscan_cluster.fit_predict(tsne_results)\n",
        "print(\"The silhouette score of the DBSCAN solution: {}\"\n",
        "      .format(metrics.silhouette_score(tsne_results, clusters, metric='euclidean')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DgFoz8sXXbd"
      },
      "source": [
        "Increased minimum sample size made performance even worse in our new clusters. Let's try adjusting epsilon value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwG_7mjbSZwx",
        "outputId": "57f25cff-96e3-4b07-a677-81f28bb70279"
      },
      "source": [
        "dbscan_cluster = DBSCAN(eps=.2, min_samples=50)\n",
        "clusters = dbscan_cluster.fit_predict(tsne_results)\n",
        "print(\"The silhouette score of the DBSCAN solution: {}\"\n",
        "      .format(metrics.silhouette_score(tsne_results, clusters, metric='euclidean')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VMPJvJdXvF0"
      },
      "source": [
        "Performance increased with epsilon value increase but it is still performing very poorly. Let's try increasing minimum sample value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgWMOpuvSiaf",
        "outputId": "e0833e13-10fd-4d6a-8650-4447796eb0d6"
      },
      "source": [
        "dbscan_cluster = DBSCAN(eps=.2, min_samples=100)\n",
        "clusters = dbscan_cluster.fit_predict(tsne_results)\n",
        "print(\"The silhouette score of the DBSCAN solution: {}\"\n",
        "      .format(metrics.silhouette_score(tsne_results, clusters, metric='euclidean')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Boqbq3YeYzNe"
      },
      "source": [
        "It seems we need to minimize our minimum sample value instead, let's try a value of 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_SqQaAbYGS1",
        "outputId": "e211bc8b-73a6-4bd3-8768-141136d5efab"
      },
      "source": [
        "dbscan_cluster = DBSCAN(eps=.2, min_samples=2)\n",
        "clusters = dbscan_cluster.fit_predict(tsne_results)\n",
        "print(\"The silhouette score of the DBSCAN solution: {}\"\n",
        "      .format(metrics.silhouette_score(tsne_results, clusters, metric='euclidean')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6xQhmpMZDcP"
      },
      "source": [
        "Much better performance than our initial DBSCAN clusters, almost perfect clustering. Let's try a couple of other epsilon values to confirm this is the right choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG1eGwCaYKL0",
        "outputId": "9f64d35a-2f28-4139-b4f3-967b08ec7a09"
      },
      "source": [
        "dbscan_cluster = DBSCAN(eps=.4, min_samples=2)\n",
        "clusters = dbscan_cluster.fit_predict(tsne_results)\n",
        "print(\"The silhouette score of the DBSCAN solution: {}\"\n",
        "      .format(metrics.silhouette_score(tsne_results, clusters, metric='euclidean')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5u_hT1uZyih",
        "outputId": "6010ea17-a9a8-47c7-a723-385c50abd7b8"
      },
      "source": [
        "dbscan_cluster = DBSCAN(eps=.3, min_samples=2)\n",
        "clusters = dbscan_cluster.fit_predict(tsne_results)\n",
        "print(\"The silhouette score of the DBSCAN solution: {}\"\n",
        "      .format(metrics.silhouette_score(tsne_results, clusters, metric='euclidean')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "folt_MbofvZo"
      },
      "source": [
        "In conclusion, we saw the best clustering performance with DBSCAN clustering using t-SNE reduced features with tuned hyperparameter values. By tuning these we were able to transform a poor clustering algorithm into one that outperformed our inital KMeans clustering algorithm. However, the number of clusters formed is significantly higher than with KMeans."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soTs4eS9Zjwv"
      },
      "source": [
        "### DBSCAN Cluster Analysis with t-SNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUcqnAAOZnER",
        "outputId": "5f529df4-0f21-4cd0-f124-deb7ff5bee65"
      },
      "source": [
        "# Finding the unique values listed in newly created DBSCAN clusters.\n",
        "np.unique(clusters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWCZ7gQDam8d"
      },
      "source": [
        "This clustering technique would prove valuable if we could find a specific cluster to target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "w-Y5BmKY2Ksv",
        "outputId": "2238fa12-baaa-4f72-c7f0-aca0c37f75e1"
      },
      "source": [
        "# Visualizing DBSCAN clustering.\n",
        "plt.figure(figsize=(11,7))\n",
        "labels = dbscan_cluster.fit_predict(tsne_components)\n",
        "plt.scatter(tsne_components[0],tsne_components[1], c=labels); \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE2CXd-pSslV"
      },
      "source": [
        "The clustering seen here conforms more to the shape of reduced dimensional t-SNE data than the KMeans clustering which conforms more to the overall structure of the data. We have defined almost 43 individual clusters with a higher silhouette score showing that DBSCAN was able to more efficiently group data than KMeans although at a higher level of complexity. Further research could include different perplexity values to attempt reducing cluster number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_p5BMFDf15D"
      },
      "source": [
        "# Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_E29f5Tr0pm"
      },
      "source": [
        "# Create our target variable. \n",
        "y = df['late']\n",
        "\n",
        "# Create X values by dropping our target from DataFrame.\n",
        "X = df.drop(columns=['late'])\n",
        "\n",
        "# Use pandas get_dummies function to create dummy variables necessary for statistical interpretation (One Hot Encoding).\n",
        "X = pd.get_dummies(X)\n",
        "\n",
        "# Implementing class balancing.\n",
        "sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
        "\n",
        "X, y = sm.fit_resample(X, y)\n",
        "\n",
        "# Scaling data.\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(X)\n",
        "\n",
        "# Splitting data into training and test sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV7QjCTHyhg4",
        "outputId": "c2bde597-0172-4c6a-b56b-8d4eaa42cb74"
      },
      "source": [
        "# Timing model.\n",
        "start_time = time.time()\n",
        "# Initiate sequential deep learning model.\n",
        "model = Sequential()\n",
        "# Create first dense layer, with input shape equal to DataFrame dimensions.\n",
        "model.add(Dense(128, input_shape=(790,), activation=\"relu\"))\n",
        "# Creating second dense layer with sigmoid activation for classification task.\n",
        "model.add(Dense(64, activation='sigmoid'))\n",
        "# Creating third output layer with sigmoid activation for classification task.\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "# Compiling model with adam optimizer and binary_crossentropy for clasification task.\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "# Fitting model to training data.\n",
        "model.fit(X_train, y_train, batch_size=128, epochs=20, verbose=1)\n",
        "# Print model time.\n",
        "print(\"--- %s seconds ---\" % round(time.time() - start_time, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRHcqwrVu0qY"
      },
      "source": [
        "Creating first and second dense layers with the first layer being a relu layer. The second and final output layers will be sigmoid for the classification task at hand. We will also set the optimizer to Adam an adaptive learning rate optimization algorithm. Setting verbose to 1 prints out results after each epoch to track progress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzhaQdWyl027",
        "outputId": "1c636648-80f9-4684-d601-9418fb3d63de"
      },
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoweW-S3ElD6"
      },
      "source": [
        "The test score for our deep learning model is almost perfect and test accuracy is also showing high performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mANkHvvqG9hg",
        "outputId": "bb87a6d1-4ea5-434c-c454-8d5478e893f1"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "report = classification_report(y_test, y_pred.round())\n",
        "cm = confusion_matrix(y_test, y_pred.round(), normalize=None)\n",
        "\n",
        "print(\"Classification report:\")\n",
        "print(report)\n",
        "print(\"Confusion matrix:\")\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7hVQ5eVHyCt"
      },
      "source": [
        "Classification report shows no signs of over or underfitting and is also showing high performance for deep learning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNZsLm8ZlZ4h"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aygF_H42ldgY"
      },
      "source": [
        "In conclusion a couple of supervised models stood out as top performers. This included the decision tree model with SelectKBest selected features which had .96 accuracy score and 4.3 second run time and KNN classifier using PCA feature decomposition with the best combination of run time and performance at 6 seconds and .96 accuracy score also. Decision tree classifier with PCA decomposed features performed best in terms of scoring with an accuracy score of.98 but at the cost of a longer run time of 44 seconds.\n",
        "\n",
        "Our unsupervised learning model using DBSCAN and t-SNE reduced features was able to cluster the data very effectively with a silhouette scoring of almost perfect at .89. Cluster analysis showed finely tuned clusters numbering 43. KMeans clustering algorithms were less effective on t-SNE and PCA reduced data but by only a little using PCA. We had scoring as high as .80 on KMeans clustered data using a KMeans cluster value of 4. This shows two different effective approaches to clustering, one displaying highly selective numerous cluster numbers seen with DBSCAN and t-SNE and the other showing broad clustering with a cluster number of 4 using KMeans and PCA data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF5nd6BqnFWy"
      },
      "source": [
        "TensorFlow deep learning models also offered comparable performance in classification report metrics to decision tree classifier with PCA but at a longer runtime of 65 seconds. Future research could reduce the deep learning model's run time by optimizing the number of epochs or adjusting batch size and minimizing the impact on scoring. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Senior Analysis: Operational Delay Prediction System\n",
        "\n",
        "The analysis above explores delay patterns and builds initial models. Below we\n",
        "apply **production-grade practices** for an operational ML system:\n",
        "\n",
        "| Practice | Operational impact |\n",
        "|---|---|\n",
        "| **Pipeline** | Reproducible preprocessing for real-time scoring |\n",
        "| **Stratified CV** | Ensures delay/on-time ratio is preserved in every fold |\n",
        "| **Multi-metric** | Recall on delays matters  passengers need advance warning |\n",
        "| **SHAP** | Operations team needs to know *which routes/times* drive delays |\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.model_selection import (\n",
        "    cross_validate, StratifiedKFold, train_test_split\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    classification_report, roc_auc_score,\n",
        "    ConfusionMatrixDisplay, RocCurveDisplay\n",
        ")\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    from portfolio_utils.data_loader import load_nj_transit\n",
        "    df_s = load_nj_transit()\n",
        "except Exception:\n",
        "    df_s = df.copy()\n",
        "\n",
        "# Use existing target if available, otherwise create binary delay target\n",
        "if 'status' in df_s.columns:\n",
        "    y_s = (df_s['status'] != 'on time').astype(int) if df_s['status'].dtype == 'object' else df_s['status']\n",
        "elif 'delay_minutes' in df_s.columns:\n",
        "    y_s = (df_s['delay_minutes'] > 0).astype(int)\n",
        "else:\n",
        "    y_s = y.copy() if 'y' in dir() else None\n",
        "\n",
        "num_cols = df_s.select_dtypes(include=[np.number]).columns.tolist()\n",
        "drop_targets = ['status', 'delay_minutes', 'stop_sequence', 'train_id']\n",
        "feature_cols = [c for c in num_cols if c not in drop_targets]\n",
        "X_s = df_s[feature_cols].fillna(0)\n",
        "\n",
        "if y_s is not None and len(X_s) == len(y_s):\n",
        "    print(f'Features: {X_s.shape[1]}, Samples: {X_s.shape[0]}')\n",
        "    print(f'Delay rate: {y_s.mean():.1%}')\n",
        "else:\n",
        "    print('Using preprocessed X, y from above')\n",
        "    X_s = X if 'X' in dir() else X_new\n",
        "    y_s = y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(\n",
        "    X_s, y_s, test_size=0.2, stratify=y_s, random_state=42\n",
        ")\n",
        "\n",
        "k = min(20, X_s.shape[1])\n",
        "models = {\n",
        "    'Decision Tree': Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('selector', SelectKBest(f_classif, k=k)),\n",
        "        ('clf', DecisionTreeClassifier(random_state=42)),\n",
        "    ]),\n",
        "    'Random Forest': Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('selector', SelectKBest(f_classif, k=k)),\n",
        "        ('clf', RandomForestClassifier(n_estimators=200, random_state=42)),\n",
        "    ]),\n",
        "    'Gradient Boosting': Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('selector', SelectKBest(f_classif, k=k)),\n",
        "        ('clf', GradientBoostingClassifier(random_state=42)),\n",
        "    ]),\n",
        "    'Logistic Regression': Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('selector', SelectKBest(f_classif, k=k)),\n",
        "        ('clf', LogisticRegression(max_iter=1000, random_state=42)),\n",
        "    ]),\n",
        "}\n",
        "\n",
        "scoring = {'accuracy': 'accuracy', 'precision': 'precision_weighted',\n",
        "           'recall': 'recall_weighted', 'f1': 'f1_weighted', 'roc_auc': 'roc_auc'}\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "results = {}\n",
        "for name, pipe in models.items():\n",
        "    cv_res = cross_validate(pipe, X_train_s, y_train_s, cv=cv,\n",
        "                            scoring=scoring, n_jobs=-1)\n",
        "    results[name] = {m: cv_res[f'test_{m}'].mean() for m in scoring}\n",
        "    print(f'{name}: F1={results[name][\"f1\"]:.4f}, ROC-AUC={results[name][\"roc_auc\"]:.4f}')\n",
        "\n",
        "results_df = pd.DataFrame(results).T.round(4)\n",
        "results_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "results_df.plot.bar(ax=ax, rot=0)\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Rail Delay Model Comparison: 5-Fold Stratified CV')\n",
        "ax.legend(loc='lower right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "best_name = results_df['f1'].idxmax()\n",
        "best_pipe = models[best_name]\n",
        "best_pipe.fit(X_train_s, y_train_s)\n",
        "\n",
        "y_pred_s = best_pipe.predict(X_test_s)\n",
        "y_proba_s = best_pipe.predict_proba(X_test_s)[:, 1]\n",
        "\n",
        "print(f'Best model: {best_name}')\n",
        "print()\n",
        "print(classification_report(y_test_s, y_pred_s, target_names=['On Time', 'Delayed']))\n",
        "print(f'ROC-AUC: {roc_auc_score(y_test_s, y_proba_s):.4f}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "ConfusionMatrixDisplay.from_predictions(\n",
        "    y_test_s, y_pred_s, display_labels=['On Time', 'Delayed'],\n",
        "    cmap='Blues', ax=ax1\n",
        ")\n",
        "ax1.set_title('Confusion Matrix')\n",
        "RocCurveDisplay.from_predictions(y_test_s, y_proba_s, ax=ax2, name=best_name)\n",
        "ax2.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
        "ax2.set_title('ROC Curve')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SHAP: What drives train delays?\n",
        "\n",
        "Operations managers need to know which factors are most predictive of delays\n",
        "so they can allocate resources (extra crew, schedule padding, equipment\n",
        "maintenance) to the highest-impact areas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "try:\n",
        "    import shap\n",
        "    estimator = best_pipe.named_steps['clf']\n",
        "    preprocessor = Pipeline(best_pipe.steps[:-1])\n",
        "    X_train_t = preprocessor.transform(X_train_s)\n",
        "\n",
        "    if 'selector' in best_pipe.named_steps:\n",
        "        mask = best_pipe.named_steps['selector'].get_support()\n",
        "        feature_names = np.array(X_s.columns)[mask].tolist()\n",
        "    else:\n",
        "        feature_names = X_s.columns.tolist()\n",
        "\n",
        "    sample = X_train_t[:300]\n",
        "    explainer = shap.TreeExplainer(estimator, sample)\n",
        "    shap_values = explainer.shap_values(sample)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 7))\n",
        "    shap.summary_plot(shap_values, sample, feature_names=feature_names,\n",
        "                      max_display=15, show=False)\n",
        "    plt.title('SHAP: What Drives Train Delays?')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except ImportError:\n",
        "    print('Install shap: pip install shap')\n",
        "except Exception as e:\n",
        "    print(f'SHAP analysis skipped: {e}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Operational recommendations\n",
        "\n",
        "**For NJ Transit Operations:**\n",
        "\n",
        "1. **Real-time delay prediction.** Deploy the model as a microservice that scores\n",
        "   upcoming departures. Push delay probability to the passenger app so riders can\n",
        "   plan alternatives before arriving at the station.\n",
        "\n",
        "2. **Resource allocation.** Use SHAP feature importance to identify which routes,\n",
        "   time windows, and infrastructure segments contribute most to delays. Target\n",
        "   capital investment and crew scheduling accordingly.\n",
        "\n",
        "3. **Schedule padding.** For routes with consistently high delay probability,\n",
        "   add buffer time to the published schedule. This improves on-time performance\n",
        "   metrics without requiring infrastructure changes.\n",
        "\n",
        "4. **Seasonal retraining.** Rail delays have strong seasonal patterns (weather,\n",
        "   holiday travel). Retrain quarterly with recent data to maintain accuracy.\n",
        "\n",
        "5. **Integration with clustering.** The unsupervised analysis above identifies\n",
        "   route clusters with similar delay profiles. Use these clusters to design\n",
        "   targeted intervention programs rather than one-size-fits-all approaches.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PFgRuY4Yzp9x",
        "HMxaEe5283F_",
        "Uj_DqjjQvOB_",
        "pGjqB8V2v8wD",
        "ASx5PcHrkb-P",
        "SWMfb2hFPSjB",
        "AYKQq4eDcH0O",
        "_xx8dwyudqGj"
      ],
      "name": "NJ Transit + Amtrak (NEC) Rail Performance Business Solution.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}